{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4106bb38",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import torch\n",
    "from pytorchtools import EarlyStopping\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718dc587",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SWong7923\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "579bb36f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('\\\\Users\\\\SWong7923\\\\PycharmProjects\\\\pythonProject2\\\\archive\\\\Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420803d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e813654",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce55d73b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1279457b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                         0\n",
       "ProductId                  0\n",
       "UserId                     0\n",
       "ProfileName               16\n",
       "HelpfulnessNumerator       0\n",
       "HelpfulnessDenominator     0\n",
       "Score                      0\n",
       "Time                       0\n",
       "Summary                   27\n",
       "Text                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e4d1f21",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# reviews['Summary'].replace('',np.nan,inplace=True)\n",
    "reviews.dropna(subset=['Summary', 'Text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4340d604",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                         0\n",
       "ProductId                  0\n",
       "UserId                     0\n",
       "ProfileName               16\n",
       "HelpfulnessNumerator       0\n",
       "HelpfulnessDenominator     0\n",
       "Score                      0\n",
       "Time                       0\n",
       "Summary                    0\n",
       "Text                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4878c4fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a9dcdd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\",\n",
    "' american ':\n",
    "        [\n",
    "            'amerikan'\n",
    "        ],\n",
    "\n",
    "    ' adolf ':\n",
    "        [\n",
    "            'adolf'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' hitler ':\n",
    "        [\n",
    "            'hitler'\n",
    "        ],\n",
    "\n",
    "    ' fuck':\n",
    "        [\n",
    "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
    "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
    "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
    "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
    "            'feck ', ' fux ', 'f\\*\\*', \n",
    "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck','fuk', 'wtf','fucck','f cking'\n",
    "        ],\n",
    "\n",
    "    ' ass ':\n",
    "        [\n",
    "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n",
    "                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
    "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n",
    "        ],\n",
    "\n",
    "    ' asshole ':\n",
    "        [\n",
    "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'ass hole'\n",
    "        ],\n",
    "\n",
    "    ' bitch ':\n",
    "        [\n",
    "            'b[w]*i[t]*ch', 'b!tch',\n",
    "            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
    "            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h','beetch'\n",
    "        ],\n",
    "\n",
    "    ' bastard ':\n",
    "        [\n",
    "            'ba[s|z]+t[e|a]+rd'\n",
    "        ],\n",
    "\n",
    "    ' transgender':\n",
    "        [\n",
    "            'transgender','trans gender'\n",
    "        ],\n",
    "\n",
    "    ' gay ':\n",
    "        [\n",
    "            'gay'\n",
    "        ],\n",
    "\n",
    "    ' cock ':\n",
    "        [\n",
    "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
    "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
    "        ],\n",
    "\n",
    "    ' dick ':\n",
    "        [\n",
    "            ' dick[^aeiou]', 'deek', 'd i c k','diick '\n",
    "        ],\n",
    "\n",
    "    ' suck ':\n",
    "        [\n",
    "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
    "        ],\n",
    "\n",
    "    ' cunt ':\n",
    "        [\n",
    "            'cunt', 'c u n t'\n",
    "        ],\n",
    "\n",
    "    ' bullshit ':\n",
    "        [\n",
    "            'bullsh\\*t', 'bull\\$hit','bs'\n",
    "        ],\n",
    "\n",
    "    ' homosexual':\n",
    "        [\n",
    "            'homo sexual','homosex'\n",
    "        ],\n",
    "\n",
    "    ' jerk ':\n",
    "        [\n",
    "            'jerk'\n",
    "        ],\n",
    "\n",
    "    ' idiot ':\n",
    "        [\n",
    "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots', 'i d i o t'\n",
    "        ],\n",
    "\n",
    "    ' dumb ':\n",
    "        [\n",
    "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
    "        ],\n",
    "\n",
    "    ' shit ':\n",
    "        [\n",
    "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n",
    "        ],\n",
    "\n",
    "    ' shithole ':\n",
    "        [\n",
    "            'shythole','shit hole'\n",
    "        ],\n",
    "\n",
    "    ' retard ':\n",
    "        [\n",
    "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
    "        ],\n",
    "\n",
    "    ' rape ':\n",
    "        [\n",
    "            ' raped'\n",
    "        ],\n",
    "\n",
    "    ' dumbass':\n",
    "        [\n",
    "            'dumb ass', 'dubass'\n",
    "        ],\n",
    "\n",
    "    ' asshead':\n",
    "        [\n",
    "            'butthead', 'ass head'\n",
    "        ],\n",
    "\n",
    "    ' sex ':\n",
    "        [\n",
    "            's3x', 'sexuality',\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' nigger ':\n",
    "        [\n",
    "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
    "        ],\n",
    "\n",
    "    ' shut the fuck up':\n",
    "        [\n",
    "            'stfu'\n",
    "        ],\n",
    "\n",
    "    ' pussy ':\n",
    "        [\n",
    "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n",
    "        ],\n",
    "\n",
    "    ' faggot ':\n",
    "        [\n",
    "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
    "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
    "        ],\n",
    "\n",
    "    ' motherfucker':\n",
    "        [\n",
    "            ' motha ', ' motha f', ' mother f', 'motherucker', 'mother fucker'\n",
    "        ],\n",
    "\n",
    "    ' whore ':\n",
    "        [\n",
    "            'wh\\*\\*\\*', 'w h o r e'\n",
    "        ],\n",
    "    \" White's \" : 'white'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "518a3fdc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'<br >', '', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05cfd789",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "clean_summaries = []\n",
    "for summary in reviews.Summary:\n",
    "    if not isinstance(summary, str):\n",
    "        print(summary)\n",
    "        continue\n",
    "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_texts = []\n",
    "for text in reviews.Text:\n",
    "    clean_texts.append(clean_text(text))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87ca2c55",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Review # 16\n",
      "Summary:  lots of twizzlers  just what you expect \n",
      "Content:  daughter loves twizzlers shipment six pounds really hit spot exactly would expect six packages strawberry twizzlers\n",
      "\n",
      "Clean Review # 17\n",
      "Summary:  poor taste\n",
      "Content:  love eating good watching tv looking movies sweet like transfer zip lock baggie stay fresh take time eating\n",
      "\n",
      "Clean Review # 18\n",
      "Summary:  love it \n",
      "Content:  satisfied twizzler purchase shared others enjoyed definitely ordering\n",
      "\n",
      "Clean Review # 19\n",
      "Summary:  great sweet candy \n",
      "Content:  twizzlers strawberry childhood favorite candy made lancaster pennsylvania candies inc one oldest confectionery firms united states subsidiary hershey company company established 1845 young smylie also make apple licorice twists green color blue raspberry licorice twists like all<br ><br >i keep dry cool place recommended put fridge according guinness book records longest licorice twist ever made measured 1 200 feet 370 weighted 100 pounds 45 kg made candies inc record breaking twist became guinness world record july 19 1998 product kosher thank\n",
      "\n",
      "Clean Review # 20\n",
      "Summary:  home delivered twizlers\n",
      "Content:  candy delivered fast purchased reasonable price home bound unable get store perfect\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
    "for i in range(15,20):\n",
    "    print(\"Clean Review #\",i+1)\n",
    "    print(\"Summary: \",clean_summaries[i])\n",
    "    print(\"Content: \",clean_texts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bc29f07",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beae712a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 132887\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_summaries)\n",
    "count_words(word_counts, clean_texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f11e83a8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 417195\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('\\\\Users\\\\SWong7923\\\\PycharmProjects\\\\pythonProject2\\\\archive\\\\numberbatch-en.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87f3cff2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 3870\n",
      "Percent of words that are missing from vocabulary: 2.91%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96ef93bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 132887\n",
      "Number of words we will use: 59595\n",
      "Percent of words we will use: 44.85%\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad3a9c99",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59595\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b352deb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count):\n",
    "    '''Convert words in text to an integer(the index of words in vocabulary).\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = [vocab_to_int[\"<GO>\"]]\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "       \n",
    "        sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87b0e5fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 25680389\n",
      "Total number of UNKs in headlines: 192248\n",
      "Percent of words that are UNK: 0.75%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23cbbca6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "037ac585",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "              counts\n",
      "count  568427.000000\n",
      "mean        6.181613\n",
      "std         2.657912\n",
      "min         2.000000\n",
      "25%         4.000000\n",
      "50%         6.000000\n",
      "75%         7.000000\n",
      "max        50.000000\n",
      "\n",
      "Texts:\n",
      "              counts\n",
      "count  568427.000000\n",
      "mean       42.996376\n",
      "std        42.520534\n",
      "min         2.000000\n",
      "25%        19.000000\n",
      "50%        30.000000\n",
      "75%        51.000000\n",
      "max      2086.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d889f84",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.0\n",
      "116.0\n",
      "208.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ab2bb32",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "11.0\n",
      "15.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15d9d041",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a95b8643",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472193\n",
      "472193\n"
     ]
    }
   ],
   "source": [
    "# takes a long time  , this is normal\n",
    "\n",
    "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "# Limit the length of summaries and texts based on the min and max ranges.\n",
    "# Remove reviews that include too many UNKs\n",
    "\n",
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 84\n",
    "max_summary_length = 13\n",
    "min_length = 2\n",
    "unk_text_limit = 1\n",
    "unk_summary_limit = 0\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for count, words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count]) >= min_length and\n",
    "            len(int_summaries[count]) <= max_summary_length and\n",
    "            len(int_texts[count]) >= min_length and\n",
    "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
    "            length == len(int_texts[count])\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4ef736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted_texts[-100:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24ab6377",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f43e270e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest text length: 24\n",
      "The longest text length: 83\n"
     ]
    }
   ],
   "source": [
    "# Subset the data for training\n",
    "start = 200000\n",
    "end = start + 300000\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6ac3040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<GO> usually buy go go brown rice convenience time cook use rice noticeable change pleasing smell taste hybrid rice love high quality rice <EOS>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([int_to_vocab[i] for i in sorted_texts_short[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85cefd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(sorted_texts_short) * 0.9)\n",
    "val_size = len(sorted_texts_short) - train_size\n",
    "zipped = list(zip(sorted_texts_short, sorted_summaries_short))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(zipped, [train_size,val_size])\n",
    "text_train, summ_train = zip(*train_dataset)\n",
    "text_val, summ_val = zip(*val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a57cac12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<GO> chewy seaweed <EOS>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([int_to_vocab[i] for i in summ_train[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28580eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<GO> usually buy trader joe brand love brand crispy much softer snack kids eat car actually less messy find quite tasty kids love first tried thought maybe moisture leaked opening jars realised different texture definitely grew <EOS>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([int_to_vocab[i] for i in text_train[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "972b7b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[63, 24, 33, 37, 36, 27, 67, 28, 36, 32]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x)for x in text_train[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "121bd237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_argsort(seq):\n",
    "    return sorted(range(len(seq)), key=lambda x: len(seq[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c38e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_index = len_argsort(text_train)\n",
    "text_train = [text_train[i] for i in sorted_index]\n",
    "summ_train = [summ_train[i] for i in sorted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "185efe81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x)for x in text_train[-2:-1]] # 24 - 83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cf30393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<GO> dog would anything liver treats whenever playmates walk drag owners house liver treat well fact owners buy healthy great training treat love <EOS>\n",
      "<GO> dogs love it plus it is healthy <EOS>\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "print(\" \".join([int_to_vocab[i] for i in text_train[k]]))\n",
    "print(\" \".join([int_to_vocab[i] for i in summ_train[k]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37faf98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按索引划分batch，返回的每个batch中存的是index\n",
    "def get_minibatches(n, minibatch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx+minibatch_size, n)))\n",
    "    return minibatches      # 这个会返回多批连着的bath_size个索引  \n",
    "#get_minibatches(len(train_en), 32)\n",
    "\n",
    "# 这个函数是在做数据预处理， 由于每个句子都不是一样长， 所以通过这个函数就可以把句子进行补齐， 不够长的在句子后面添加0\n",
    "def prepare_data(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]    # 得到每个句子的长度\n",
    "    n_samples = len(seqs)       # 得到一共有多少个句子\n",
    "    max_len = np.max(lengths)              # 找出最大的句子长度\n",
    "    \n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')    # 按照最大句子长度生成全0矩阵\n",
    "    x_lengths = np.array(lengths).astype('int32')\n",
    "    for idx, seq in enumerate(seqs):        # 把有句子的位置填充进去\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    return x, x_lengths      # x_mask\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)   # 得到batch个索引\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:   # 每批数据的索引\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]   # 取数据\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]  # 取数据\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences) # 填充成一样的长度， 但是要记录一下句子的真实长度， 这个在后面输入网络的时候得用\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    return all_ex\n",
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(text_train, summ_train, batch_size)   # 产生训练集\n",
    "random.shuffle(train_data)\n",
    "dev_data = gen_examples(text_val, summ_val, batch_size)   # 产生验证集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f9b44fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 43) (64,) (64, 12) (64,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data[1][0].shape, train_data[1][1].shape, train_data[1][2].shape, train_data[1][3].shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b79c5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)   # embedding layer最后输出的维度是hidden_size,相当于现在每个词的embedding维度等于hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, lengths):  \n",
    "        # 这里需要输入lengths， 因为每个句子是不一样长的，我们需要每个句子最后一个时间步的隐藏状态,所以需要知道句子有多长， x表示一个batch里面的句子 \n",
    "        # 这里的x lengths 都是tensor， 调用tensor.sort, 返回的是排好的数组和index\n",
    "        # 把batch里面的seq按照长度排序  \n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)  #d_le sorted_len表示排好序的数组， sorted_index表示每个元素在原数组位置\n",
    "        x_sorted = x[sorted_idx.long()]  # 将句子已经按照sorted_idx排序\n",
    "        embedded = self.dropout(self.embed(x_sorted))   # [batch_size, seq_len, embed_size]\n",
    "        #得到embedding， 每个句子的embedding维度等于hidden_size\n",
    "        # 下面一段代码处理变长序列\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)    # 这里的data.numpy()是原来张量的克隆， 然后转成了numpy数组， 相当于clone().numpy()\n",
    "        # 上面这句话之后， 会把变长序列的0都给去掉， 之前填充的字符都给压扁\n",
    "        packed_out, hid = self.rnn(packed_embedded)    # 当输入为packed_embedding时，句子有多长，hidden state就有多长\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True) # 将out再填充回去，便于后续处理（实际上后面直接抛弃了，有用的只是最后一个字的hidden state） \n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)      # 恢复原来的顺序\n",
    "        out = out[original_idx.long()].contiguous()  # 深拷贝rnn输出的tensor\n",
    "        hid = hid[:, original_idx.long()].contiguous() \n",
    "        \n",
    "        return out, hid[[-1]]   # 把最后一层的hid给拿出来  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6b9cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, y, y_lengths, hid):\n",
    "        # y: [batch_size, y_length] y_length是最长句子的长度（每个句子都被padding成了这么长）\n",
    "        # hid是encoder得到的最后一个字的hidden_state， 实际上包含了整个输入序列的信息\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)    # 依然是句子从长到短排序\n",
    "\n",
    "        y_sorted = y[sorted_idx.long()] # 排序过程\n",
    "        hid = hid[:, sorted_idx.long()] # 排序过程\n",
    "        \n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, y_length, embed_size] 这里的embed_size实际上是等于hidden_size的\n",
    "        \n",
    "        pack_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(pack_seq, hid)   # decoder里面的初始state 就是encoder中得到的最后一个字的state\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)   # [batch, y_length, hidden_size] 恢复out的长度，（实际上是把packed_sequence 转回tensor）\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False) #恢复原来的句子顺序\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()  # [batch, y_length, hidden_size] 恢复顺序+深拷贝\n",
    "        \n",
    "        hid = hid[:, original_idx.long()].contiguous()   # [1， batch, hidden_size] decoder的hid_state其实没用，只需要输出即可。\n",
    "        output = F.log_softmax(self.out(output_seq), -1)    \n",
    "        #   self.out 是一个fully connected neural network,输出结果是[batch, y_length, vocab_size] 然后对最后一个维度 vocab_size进行归一化,那么现在output的每一步输出，都是一个vocab_size长度的vector，取其中数值最大的维度就得到了对应的单词\n",
    "        \n",
    "        return output, hid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f6686ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(PlainSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder   \n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)  \n",
    "        output, hid = self.decoder(y, y_lengths, hid) \n",
    "        return output, None\n",
    "\n",
    "    def translate(self, x, x_lengths, y, max_length=10): # 这个是进来一个句子进行翻译, y是仅有一个起始字符<go>的decoder输入序列  max_length句子的最大长度\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)   # 将原来的summary进行解码\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):   \n",
    "            output, hid = self.decoder(y, torch.ones(batch_size).long().to(y.device), hid=hid) # 编码，此时decoder的输入就只是<go>\n",
    "            y = output.max(2)[1].view(batch_size, 1) #取dim=2 上的最大值所在的维度，即对应词在vocabulary上的index\n",
    "            preds.append(y)\n",
    "        \n",
    "        return torch.cat(preds, 1), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5c1000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked cross entropy loss\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "    \n",
    "    def forward(self, input, target, mask):\n",
    "        # input: [batch_size, seq_len, vocab_size]    每个单词的可能性\n",
    "        input = input.contiguous().view(-1, input.size(2))   # [batch_size*seq_len-1, vocab_size] 将input的batch_size和seq_len合并，再计算loss\n",
    "        target = target.contiguous().view(-1, 1)    #  [batch_size*seq_len-1, 1] # target里dim=1存的是正确summary 的index\n",
    "        \n",
    "        mask = mask.contiguous().view(-1, 1)   # [batch_size*seq_len-1, 1]\n",
    "        output = -input.gather(1, target) * mask # 在每个vocab_size维度取正确单词的索引， 但是里面有很多是填充进去的， 所以mask去掉这些填充的， \n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "        \n",
    "        return output  # [batch_size*seq_len-1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cdef7494",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dropout = 0.2\n",
    "hidden_size = 100\n",
    "encoder = PlainEncoder(vocab_size=len(word_embedding_matrix), hidden_size=hidden_size, dropout=dropout)\n",
    "decoder = PlainDecoder(vocab_size=len(word_embedding_matrix), hidden_size=hidden_size, dropout=dropout)\n",
    "\n",
    "model = PlainSeq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d774435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/50] train_loss: 4.90137 valid_loss: 4.35164\n",
      "Validation loss decreased (inf --> 4.351643).  Saving model ...\n",
      "[ 2/50] train_loss: 4.16238 valid_loss: 4.03779\n",
      "Validation loss decreased (4.351643 --> 4.037790).  Saving model ...\n",
      "[ 3/50] train_loss: 3.88608 valid_loss: 3.87231\n",
      "Validation loss decreased (4.037790 --> 3.872313).  Saving model ...\n",
      "[ 4/50] train_loss: 3.70633 valid_loss: 3.76332\n",
      "Validation loss decreased (3.872313 --> 3.763320).  Saving model ...\n",
      "[ 5/50] train_loss: 3.57251 valid_loss: 3.68436\n",
      "Validation loss decreased (3.763320 --> 3.684358).  Saving model ...\n",
      "[ 6/50] train_loss: 3.46747 valid_loss: 3.62639\n",
      "Validation loss decreased (3.684358 --> 3.626387).  Saving model ...\n",
      "[ 7/50] train_loss: 3.38066 valid_loss: 3.58009\n",
      "Validation loss decreased (3.626387 --> 3.580092).  Saving model ...\n",
      "[ 8/50] train_loss: 3.30689 valid_loss: 3.54176\n",
      "Validation loss decreased (3.580092 --> 3.541759).  Saving model ...\n",
      "[ 9/50] train_loss: 3.24313 valid_loss: 3.50818\n",
      "Validation loss decreased (3.541759 --> 3.508176).  Saving model ...\n",
      "[10/50] train_loss: 3.18685 valid_loss: 3.48206\n",
      "Validation loss decreased (3.508176 --> 3.482062).  Saving model ...\n",
      "[11/50] train_loss: 3.13619 valid_loss: 3.46005\n",
      "Validation loss decreased (3.482062 --> 3.460050).  Saving model ...\n",
      "[12/50] train_loss: 3.08796 valid_loss: 3.43860\n",
      "Validation loss decreased (3.460050 --> 3.438602).  Saving model ...\n",
      "[13/50] train_loss: 3.04600 valid_loss: 3.42399\n",
      "Validation loss decreased (3.438602 --> 3.423986).  Saving model ...\n",
      "[14/50] train_loss: 3.00603 valid_loss: 3.40830\n",
      "Validation loss decreased (3.423986 --> 3.408301).  Saving model ...\n",
      "[15/50] train_loss: 2.97129 valid_loss: 3.39648\n",
      "Validation loss decreased (3.408301 --> 3.396478).  Saving model ...\n",
      "[16/50] train_loss: 2.93534 valid_loss: 3.38575\n",
      "Validation loss decreased (3.396478 --> 3.385750).  Saving model ...\n",
      "[17/50] train_loss: 2.90447 valid_loss: 3.37478\n",
      "Validation loss decreased (3.385750 --> 3.374776).  Saving model ...\n",
      "[18/50] train_loss: 2.87437 valid_loss: 3.36544\n",
      "Validation loss decreased (3.374776 --> 3.365441).  Saving model ...\n",
      "[19/50] train_loss: 2.84628 valid_loss: 3.35680\n",
      "Validation loss decreased (3.365441 --> 3.356797).  Saving model ...\n",
      "[20/50] train_loss: 2.82043 valid_loss: 3.35055\n",
      "Validation loss decreased (3.356797 --> 3.350550).  Saving model ...\n",
      "[21/50] train_loss: 2.79406 valid_loss: 3.34675\n",
      "Validation loss decreased (3.350550 --> 3.346747).  Saving model ...\n",
      "[22/50] train_loss: 2.77008 valid_loss: 3.33842\n",
      "Validation loss decreased (3.346747 --> 3.338416).  Saving model ...\n",
      "[23/50] train_loss: 2.74808 valid_loss: 3.33490\n",
      "Validation loss decreased (3.338416 --> 3.334900).  Saving model ...\n",
      "[24/50] train_loss: 2.72596 valid_loss: 3.32854\n",
      "Validation loss decreased (3.334900 --> 3.328543).  Saving model ...\n",
      "[25/50] train_loss: 2.70421 valid_loss: 3.32505\n",
      "Validation loss decreased (3.328543 --> 3.325051).  Saving model ...\n",
      "[26/50] train_loss: 2.68471 valid_loss: 3.32109\n",
      "Validation loss decreased (3.325051 --> 3.321089).  Saving model ...\n",
      "[27/50] train_loss: 2.66487 valid_loss: 3.31773\n",
      "Validation loss decreased (3.321089 --> 3.317732).  Saving model ...\n",
      "[28/50] train_loss: 2.64692 valid_loss: 3.31328\n",
      "Validation loss decreased (3.317732 --> 3.313282).  Saving model ...\n",
      "[29/50] train_loss: 2.62932 valid_loss: 3.31242\n",
      "Validation loss decreased (3.313282 --> 3.312417).  Saving model ...\n",
      "[30/50] train_loss: 2.61270 valid_loss: 3.30916\n",
      "Validation loss decreased (3.312417 --> 3.309161).  Saving model ...\n",
      "[31/50] train_loss: 2.59521 valid_loss: 3.30584\n",
      "Validation loss decreased (3.309161 --> 3.305836).  Saving model ...\n",
      "[32/50] train_loss: 2.57955 valid_loss: 3.30323\n",
      "Validation loss decreased (3.305836 --> 3.303233).  Saving model ...\n",
      "[33/50] train_loss: 2.56292 valid_loss: 3.30446\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[34/50] train_loss: 2.56230 valid_loss: 3.30605\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[35/50] train_loss: 2.56256 valid_loss: 3.30413\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[36/50] train_loss: 2.56339 valid_loss: 3.30499\n",
      "EarlyStopping counter: 4 out of 5\n",
      "[37/50] train_loss: 2.56330 valid_loss: 3.30472\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# 定义训练和验证函数\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "#Please see the link below to learn about early stopping\n",
    "#https://towardsdatascience.com/a-practical-introduction-to-early-stopping-in-machine-learning-550ac88bc8fd \n",
    "min_val_loss = 100\n",
    "early_stop = False\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()    # 这个是一个batch的英文句子 大小是[batch_size, seq_len]\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()    # 每个句子的长度\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()  # decoder的输入，训练的时候是整个summary（带<GO>）\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()   # 解码器那边的输出  [batch_size, seq_len-1]去掉<GO>\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()  # 这个减去1， 去掉初始符号 [batch_size, seq_len-1]\n",
    "            mb_y_len[mb_y_len<=0] =  1   # 以防出错，防止trainset里面的summary长度被记为0（实际上不可能）\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            \n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]  \n",
    "            # [batch_size,最长句子的长度], 上面是bool类型， 下面是float类型， 只计算每个句子的有效部分， 填充的那部分去掉\n",
    "            mb_out_mask = mb_out_mask.float()  # [batch_size, seq_len-1]  因为mb_y_len.max()就是seq_len-1\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            \n",
    "    #print('Validation loss', total_loss / total_num_words)\n",
    "    val_loss = total_loss / total_num_words;\n",
    "    val_losses.append(val_loss)\n",
    "    return val_loss\n",
    "\n",
    "def train(model, data, num_epochs, patience):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            \n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            \n",
    "            # 更新\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)     # 防止梯度爆炸\n",
    "            optimizer.step()\n",
    "            \n",
    "            #if it % 100 == 0:\n",
    "            #   print('Epoch', epoch, 'iteration', it, 'loss', loss.item())\n",
    "       \n",
    "        #print('Epoch', epoch, 'Training loss', total_loss / total_num_words)\n",
    "        trainingloss = total_loss / total_num_words;\n",
    "        train_losses.append(trainingloss)\n",
    "        #if epoch % 5 == 0:\n",
    "        val_loss=evaluate(model, dev_data)\n",
    "        \n",
    "        epoch_len = len(str(num_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {trainingloss:.5f} ' +\n",
    "                     f'valid_loss: {val_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        early_stopping(val_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "        model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "            \n",
    "train(model, train_data, num_epochs=50, patience=5)\n",
    "#print(train_losses)\n",
    "#print(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c007c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# epoch = range(num_epochs)\n",
    "# fig, ax = plt.subplots()\n",
    "# plt.plot(epoch, train_losses, 'g', label='Training loss')\n",
    "# plt.plot(epoch, val_losses, 'b', label='Validation loss')\n",
    "# plt.title('Training and Validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# ax.set_xticks(epoch)\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    " # visualize the loss as the network trained\n",
    "fig = plt.figure()\n",
    "plt.plot(range(1,len(train_losses)+1),train_losses, label='Training Loss')\n",
    "plt.plot(range(1,len(val_losses)+1),val_losses,label='Validation Loss')\n",
    "\n",
    "# find position of lowest validation loss\n",
    "minposs = val_losses.index(min(val_losses))+1 \n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "#plt.ylim(1, 5) # consistent scale\n",
    "plt.xlim(0, len(train_losses)+1) # consistent scale\n",
    "#plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.title('Training and Validation loss')\n",
    "plt.show()\n",
    "fig.savefig('loss_plot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c99a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_dev(i):\n",
    "    text = \" \".join([int_to_vocab[w] for w in text_val[i]])\n",
    "    print(text)\n",
    "    summary = \" \".join([int_to_vocab[w] for w in summ_val[i]])\n",
    "    print(summary)\n",
    "\n",
    "    mb_x = torch.from_numpy(np.array(text_val[i]).reshape(1,-1)).long().to(device)\n",
    "    mb_x_len = torch.from_numpy(np.array([len(text_val[i])])).long().to(device)\n",
    "    # mb_y = torch.from_numpy(summ_val[i].reshape(1,-1)).long().to(device)\n",
    "    # mb_y_len = torch.from_numpy([len(summ_val[i])]).long().to(device)\n",
    "    bos = torch.Tensor([[vocab_to_int[\"<GO>\"]]]).long().to(device)\n",
    "    # 翻译时decoder的输入序列bos： [<GO>的index] \n",
    "    translation, attn = model.translate(mb_x,mb_x_len, bos)\n",
    "    translation = [int_to_vocab[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word !=\"<EOS>\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(\" \".join(trans))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bda18b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<GO> chihuahua since puppy never much appetite tried varieties dry canned dog food would barely sniff underweight result coat looked terrible glad discover dog food thing eat since began feeding lamb gained whole pound coat thicker longer also eats wellness dry really cannot say enough good things brand dog food also works chihuahua huge appetite severe food allergies <br ><br >if nothing else worked give try may surprised much dog loves <EOS>\n",
      "<GO> this is the only food my dog will eat <EOS>\n",
      "my dog loves it\n",
      "\n",
      "<GO> absolute favorite yogi tea pleasantly spicy hint rich chocolate much prefer mayan cocoa spice sweet thai flavors reach get craving cup hot chocolate want calories <EOS>\n",
      "<GO> great warming flavor on a dreary day <EOS>\n",
      "hot cocoa\n",
      "\n",
      "<GO> found sharp raw product works well difficult combine ingredients chocolate overpowering tried infusing herbs spices none showed chocolate tight composed earthy tones somehow anything add serves bring chocolate textures flavor found hard tone much less balance works well base chocolate souffle <EOS>\n",
      "<GO> intense product <EOS>\n",
      "great product\n",
      "\n",
      "<GO> dad fallen ill live close purchased coffees electric kettle still able enjoy coffee without hassle got 3 boxes wish ordered visitors fill kettle pour yummie cup keep getting asked purchased everyone amazed simplicity cup jo way like hurry thanks ordering <EOS>\n",
      "<GO> yummie <EOS>\n",
      "best coffee ever\n",
      "\n",
      "<GO> stash english breakfast tea several times two years since ordered 100 amazon know got bad batch tea seem good use tastes tiny bit better say tetley lipton remember far superior know stale expiration date end 2014 wish picked another brand <EOS>\n",
      "<GO> kinda disapointed <EOS>\n",
      "not as good as tazo\n",
      "\n",
      "<GO> one brands treats appropriately small enough <UNK> mix owner large dog would also great training large dogs dogs really sense size happy small bite much larger treat want dog busy chewing want keep attention repeat behavior treats right size <EOS>\n",
      "<GO> great for training <EOS>\n",
      "great for training\n",
      "\n",
      "<GO> hooked thai tea stuff crazy good super easy make make 2 large glasses place 8 tablespoons tea french press coffee pot pour 2 cups boiling water brewing add sweetened condensed milk glasses 2 3 tbsp sugar press tea filter pour 1 cup glass stir fill ice go best stuff ever <EOS>\n",
      "<GO> crazy good stuff <EOS>\n",
      "soothing and soothing\n",
      "\n",
      "<GO> excited try cat litter hoping eliminate feces odor litters failed litter help odor gosh dust horrible clean box twice day time dust envelopes head start coughing would rather smell poopy breath whatever product bad bad bad cat box covered poor cat must also suffering going try corn based product next told people use horse bedding chicken crumbles make fine cat litter inexpensive <EOS>\n",
      "<GO> cough choke gag <EOS>\n",
      "did not work for my cat\n",
      "\n",
      "<GO> husband enjoys tea appreciate easy make particularly considering voracious appetite stuff personally enjoy tea make tea found job little easier would great camping picnics power outages wherever may water necessarily hot water <EOS>\n",
      "<GO> easy to make <EOS>\n",
      "great tea\n",
      "\n",
      "<GO> wife soft fine blondish hair enjoy product directions non existent awful says simply apply wet dry hair product page instructions simply clean apply first time using product better explanations applying removing would nice much hair stopped using applied wet hair looked greasy dried short asian hair tried also strong coconut smell tolerable used shower washed hair still feels greasy washing used clear line shampoos like fan <EOS>\n",
      "<GO> makes you look greasy <EOS>\n",
      "smells like hair\n",
      "\n",
      "<GO> giving 2 day dog long time loves morning walk runs kitchen get two pedigree treats one milkbone sure knows <UNK> <br ><br >problem wal mart carry getting kroger yesterday closeout sale told kroger vendor carrying anymore start ordering line <br ><br >anyway dog love <EOS>\n",
      "<GO> my dog loves these treats <EOS>\n",
      "yummy\n",
      "\n",
      "<GO> stuff nasty even think particularly good instant would anyone spend money keurig coffee maker order make instant coffee home need kettle microwave waste space kitchen landfill worst part know instant taste look carefully package see water soluble code words instant open one see <EOS>\n",
      "<GO> why use a brewer for instant coffee <EOS>\n",
      "instant coffee\n",
      "\n",
      "<GO> recommend using grinder coarse salt dissolve well cooking kosher salt also strong salty taste fine expecting smoky taste <br ><br >good quality worth money opinion <EOS>\n",
      "<GO> good and salty <EOS>\n",
      "good but not great\n",
      "\n",
      "<GO> best tasting fish taco mix ever want taste flavors also got nice medium zesty taste hot mild best part <UNK> love cannot beat sure <EOS>\n",
      "<GO> even the kids love it <EOS>\n",
      "best taco seasoning i have ever had\n",
      "\n",
      "<GO> love flavor tea smell bag strong made almost best tasting tea ever worth price one tea use lot sugar still enjoy taste always toss tea beautifully done white ambrosia made white lion <EOS>\n",
      "<GO> this is my 2nd favorite tea <EOS>\n",
      "great tea\n",
      "\n",
      "<GO> post review spry gum ridiculous thought getting great deal suppose get pay eat two every time flavor gone literally within one minute probably even bit generous would recommend gum anyone cinnamon flavor amazing initially said flavor lasts briefly worth getting want chew something go ahead buy expect flavor <EOS>\n",
      "<GO> great taste for about 5 seconds <EOS>\n",
      "not the best\n",
      "\n",
      "<GO> much better tabasco try tabasco tastes much like vinegar <br >because yucatan sunshine uses habanero peppers might worried hot actually bit mild <UNK> opinion <br >it simply great flavor like put drops tortilla chip forget salsa need <EOS>\n",
      "<GO> do not fear the habanero <EOS>\n",
      "great flavor\n",
      "\n",
      "<GO> tasty fun snacks easy take along backpack bag pretty healthful probably wonderful kids would caution adults bit careful broke tooth chomping one still buy much cautious nibbling <EOS>\n",
      "<GO> fruit ropes good snack <EOS>\n",
      "tasty but too salty\n",
      "\n",
      "<GO> disappointed tea smell anything green tea flavor tried roast brown rice add taste could taste green tea like haystack amazon nice sent another 2 bags result taste recommand product <EOS>\n",
      "<GO> do not have any taste <EOS>\n",
      "not as good as i expected\n",
      "\n",
      "<GO> mind taste sucralose think nuclear fallout combined lye like drink also begs question first ingredient listed sugar even felt need add sucralose <EOS>\n",
      "<GO> ewwww nasty <EOS>\n",
      "not as good as the original\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(110,130):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3522ab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<GO> really nice almond flour fine like says dark pieces skins like flours use bake macarons great texture color two pack sure buying soon <EOS>\n",
      "<GO> great for macarons <EOS>\n",
      "nice and tasty\n"
     ]
    }
   ],
   "source": [
    "translate_dev(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4989caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), '\\\\Users\\\\SWong7923\\\\PycharmProjects\\\\pythonProject2\\\\model\\\\checkpoint.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c36000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('\\\\Users\\\\SWong7923\\\\PycharmProjects\\\\pythonProject2\\\\checkpoint.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
