{"cells":[{"cell_type":"code","execution_count":127,"id":"6c8fbae8","metadata":{"pycharm":{"name":"#%%\n"},"id":"6c8fbae8","executionInfo":{"status":"ok","timestamp":1670189904278,"user_tz":360,"elapsed":374,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import pickle\n","from torch.utils.data import Dataset, DataLoader\n","from torch.autograd import Variable\n","import torch\n","import torch.nn.utils.rnn as rnn_utils\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torchvision.utils import make_grid\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import random \n","import time"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yXdM1XSnu83-","executionInfo":{"status":"ok","timestamp":1670189909401,"user_tz":360,"elapsed":1678,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"a383a737-fcf6-4a6f-daf4-3862c720ee2d"},"id":"yXdM1XSnu83-","execution_count":128,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":129,"id":"b0b2a6bd","metadata":{"id":"b0b2a6bd","executionInfo":{"status":"ok","timestamp":1670189914002,"user_tz":360,"elapsed":1943,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["# Subset the data for training\n","with open('/content/drive/MyDrive/Colab Notebooks/CSC592/testWTF/tem_var.pkl', 'rb') as f:\n","    sorted_texts_short = pickle.load(f)\n","    sorted_summaries_short = pickle.load(f)\n","    vocab_to_int = pickle.load(f)\n","    int_to_vocab = pickle.load(f)\n","    word_embedding_matrix = pickle.load(f)"]},{"cell_type":"code","source":["pretrained_embedding = torch.FloatTensor(word_embedding_matrix)\n","print(pretrained_embedding.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NdJTSDgwEL8M","executionInfo":{"status":"ok","timestamp":1670189917100,"user_tz":360,"elapsed":3,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"a63da2bc-2fa7-411c-e394-d140f40f79ac"},"id":"NdJTSDgwEL8M","execution_count":130,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([60433, 300])\n"]}]},{"cell_type":"code","execution_count":131,"id":"ce2621e4","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"ce2621e4","executionInfo":{"status":"ok","timestamp":1670189920891,"user_tz":360,"elapsed":333,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"a7d94c2e-6a30-4baa-ba62-6e5807c9e773"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<GO> makes perfect crab cake right recipe back packet made 3 instead 6 recommend broiled golden brown good g ms baltimore saying lot <EOS>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":131}],"source":["\" \".join([int_to_vocab[i] for i in sorted_texts_short[2]])"]},{"cell_type":"code","execution_count":132,"id":"75b2001a","metadata":{"id":"75b2001a","executionInfo":{"status":"ok","timestamp":1670189926090,"user_tz":360,"elapsed":1334,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["train_size = int(len(sorted_texts_short) * 0.9)\n","val_size = len(sorted_texts_short) - train_size\n","zipped = list(zip(sorted_texts_short, sorted_summaries_short))\n","\n","\n","\n"]},{"cell_type":"code","execution_count":133,"id":"227ef832","metadata":{"id":"227ef832","executionInfo":{"status":"ok","timestamp":1670189928903,"user_tz":360,"elapsed":319,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["train_dataset, val_dataset = torch.utils.data.random_split(zipped, [train_size,val_size])"]},{"cell_type":"code","execution_count":134,"id":"36ad2ee8","metadata":{"id":"36ad2ee8","executionInfo":{"status":"ok","timestamp":1670189934590,"user_tz":360,"elapsed":309,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["text_train, summ_train = zip(*train_dataset)"]},{"cell_type":"code","execution_count":135,"id":"25858526","metadata":{"id":"25858526","executionInfo":{"status":"ok","timestamp":1670189936637,"user_tz":360,"elapsed":2,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["text_val, summ_val = zip(*val_dataset)"]},{"cell_type":"code","execution_count":136,"id":"8f63e855","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"8f63e855","executionInfo":{"status":"ok","timestamp":1670189938585,"user_tz":360,"elapsed":5,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"2c2671c6-eea1-4f76-a408-ad4a02597e3e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<GO> serious chocolate at a bargain price <EOS>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":136}],"source":["\" \".join([int_to_vocab[i] for i in summ_train[3]])"]},{"cell_type":"code","execution_count":137,"id":"e6135d93","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"e6135d93","executionInfo":{"status":"ok","timestamp":1670189941554,"user_tz":360,"elapsed":471,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"a1ae1efc-7125-4816-e962-21994d9c2c4c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<GO> excellent deal like nutella local grocer carry even cases good buy even shipping costs included number reviews nutella available amazon decadently rich chocolate hazelnut spread needs refrigeration wonderful everything croissants graham crackers fruit 6 pack ensures plenty hand cost per jar including shipping charges actually less local grocer charges <EOS>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":137}],"source":["\" \".join([int_to_vocab[i] for i in text_train[3]])"]},{"cell_type":"code","execution_count":138,"id":"ca2cdc34","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca2cdc34","executionInfo":{"status":"ok","timestamp":1670189952409,"user_tz":360,"elapsed":516,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"eb873c4e-4b3e-4794-cfbd-aafacab43e68"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[44, 26, 38, 51, 66, 77, 26, 46, 27, 26]"]},"metadata":{},"execution_count":138}],"source":["[len(x)for x in text_train[:10]]"]},{"cell_type":"code","execution_count":139,"id":"2a45274b","metadata":{"id":"2a45274b","executionInfo":{"status":"ok","timestamp":1670189954522,"user_tz":360,"elapsed":4,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["def len_argsort(seq):\n","    return sorted(range(len(seq)), key=lambda x: len(seq[x]))"]},{"cell_type":"code","execution_count":140,"id":"93af0dcf","metadata":{"id":"93af0dcf","executionInfo":{"status":"ok","timestamp":1670189957240,"user_tz":360,"elapsed":2,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["sorted_index = len_argsort(text_train)\n","text_train = [text_train[i] for i in sorted_index]\n","summ_train = [summ_train[i] for i in sorted_index]"]},{"cell_type":"code","execution_count":141,"id":"6b69b98c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6b69b98c","executionInfo":{"status":"ok","timestamp":1670189964180,"user_tz":360,"elapsed":303,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"113b75fe-00c5-43e0-b051-2cd9525b1521"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[83]"]},"metadata":{},"execution_count":141}],"source":["[len(x)for x in text_train[-2:-1]] # 24 - 83"]},{"cell_type":"code","execution_count":142,"id":"be85eef4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"be85eef4","executionInfo":{"status":"ok","timestamp":1670189967050,"user_tz":360,"elapsed":513,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"15d3fd84-c6d3-46db-9fca-83022a28b12a"},"outputs":[{"output_type":"stream","name":"stdout","text":["<GO> absolutely loved product mainly used fondant modelling soft hold shape adding gum tex helped hold shape dry hard taste good tasted wanted <EOS>\n","<GO> excellent product <EOS>\n"]}],"source":["k = 2\n","print(\" \".join([int_to_vocab[i] for i in text_train[k]]))\n","print(\" \".join([int_to_vocab[i] for i in summ_train[k]]))"]},{"cell_type":"code","execution_count":143,"id":"acfec761","metadata":{"id":"acfec761","executionInfo":{"status":"ok","timestamp":1670189971656,"user_tz":360,"elapsed":1843,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["# 按索引划分batch，返回的每个batch中存的是index\n","def get_minibatches(n, minibatch_size, shuffle=True):\n","    idx_list = np.arange(0, n, minibatch_size)\n","    if shuffle:\n","        np.random.shuffle(idx_list)\n","    minibatches = []\n","    for idx in idx_list:\n","        minibatches.append(np.arange(idx, min(idx+minibatch_size, n)))\n","    return minibatches      # 这个会返回多批连着的bath_size个索引  \n","#get_minibatches(len(train_en), 32)\n","\n","# 这个函数是在做数据预处理， 由于每个句子都不是一样长， 所以通过这个函数就可以把句子进行补齐， 不够长的在句子后面添加0\n","def prepare_data(seqs):\n","    lengths = [len(seq) for seq in seqs]    # 得到每个句子的长度\n","    n_samples = len(seqs)       # 得到一共有多少个句子\n","    max_len = np.max(lengths)              # 找出最大的句子长度\n","    \n","    x = np.zeros((n_samples, max_len)).astype('int32')    # 按照最大句子长度生成全0矩阵\n","    x_lengths = np.array(lengths).astype('int32')\n","    for idx, seq in enumerate(seqs):        # 把有句子的位置填充进去\n","        x[idx, :lengths[idx]] = seq\n","    return x, x_lengths      # x_mask\n","\n","def gen_examples(en_sentences, cn_sentences, batch_size):\n","    minibatches = get_minibatches(len(en_sentences), batch_size)   # 得到batch个索引\n","    all_ex = []\n","    for minibatch in minibatches:   # 每批数据的索引\n","        mb_en_sentences = [en_sentences[t] for t in minibatch]   # 取数据\n","        mb_cn_sentences = [cn_sentences[t] for t in minibatch]  # 取数据\n","        mb_x, mb_x_len = prepare_data(mb_en_sentences) # 填充成一样的长度， 但是要记录一下句子的真实长度， 这个在后面输入网络的时候得用\n","        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n","        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n","    return all_ex\n","\n","batch_size = 64\n","train_data = gen_examples(text_train, summ_train, batch_size)   # 产生训练集\n","random.shuffle(train_data)\n","dev_data = gen_examples(text_val, summ_val, batch_size)   # 产生验证集\n"]},{"cell_type":"code","execution_count":144,"id":"46c066b6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46c066b6","executionInfo":{"status":"ok","timestamp":1670189975653,"user_tz":360,"elapsed":311,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"e3217746-75db-4e58-e5de-d534c0bd59de"},"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 26) (64,) (64, 11) (64,)\n"]}],"source":["print(train_data[1][0].shape, train_data[1][1].shape, train_data[1][2].shape, train_data[1][3].shape)  "]},{"cell_type":"code","execution_count":145,"id":"eb511065","metadata":{"id":"eb511065","executionInfo":{"status":"ok","timestamp":1670189978787,"user_tz":360,"elapsed":295,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["class PlainEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n","        super(PlainEncoder, self).__init__()\n","        self.embed = nn.Embedding.from_pretrained(pretrained_embedding)\n","        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)   # embedding layer最后输出的维度是hidden_size,相当于现在每个词的embedding维度等于hidden_size\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, x, lengths):  \n","        # 这里需要输入lengths， 因为每个句子是不一样长的，我们需要每个句子最后一个时间步的隐藏状态,所以需要知道句子有多长， x表示一个batch里面的句子 \n","        # 这里的x lengths 都是tensor， 调用tensor.sort, 返回的是排好的数组和index\n","        # 把batch里面的seq按照长度排序  \n","        sorted_len, sorted_idx = lengths.sort(0, descending=True)  #d_le sorted_len表示排好序的数组， sorted_index表示每个元素在原数组位置\n","        x_sorted = x[sorted_idx.long()]  # 将句子已经按照sorted_idx排序\n","        embedded = self.dropout(self.embed(x_sorted))   # [batch_size, seq_len, embed_size]\n","        #得到embedding， 每个句子的embedding维度等于hidden_size\n","        # 下面一段代码处理变长序列\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)    # 这里的data.numpy()是原来张量的克隆， 然后转成了numpy数组， 相当于clone().numpy()\n","        # 上面这句话之后， 会把变长序列的0都给去掉， 之前填充的字符都给压扁\n","        packed_out, hid = self.rnn(packed_embedded)    # 当输入为packed_embedding时，句子有多长，hidden state就有多长\n","        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True) # 将out再填充回去，便于后续处理（实际上后面直接抛弃了，有用的只是最后一个字的hidden state） \n","        _, original_idx = sorted_idx.sort(0, descending=False)      # 恢复原来的顺序\n","        out = out[original_idx.long()].contiguous()  # 深拷贝rnn输出的tensor\n","        hid = hid[:, original_idx.long()].contiguous() \n","        \n","        return out, hid[[-1]]   # 把最后一层的hid给拿出来  "]},{"cell_type":"code","execution_count":146,"id":"911620d0","metadata":{"id":"911620d0","executionInfo":{"status":"ok","timestamp":1670189985102,"user_tz":360,"elapsed":337,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["class PlainDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n","        super(PlainDecoder, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, hidden_size)\n","        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","    \n","    def forward(self, y, y_lengths, hid):\n","        # y: [batch_size, y_length] y_length是最长句子的长度（每个句子都被padding成了这么长）\n","        # hid是encoder得到的最后一个字的hidden_state， 实际上包含了整个输入序列的信息\n","        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)    # 依然是句子从长到短排序\n","\n","        y_sorted = y[sorted_idx.long()] # 排序过程\n","        hid = hid[:, sorted_idx.long()] # 排序过程\n","        \n","        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, y_length, embed_size] 这里的embed_size实际上是等于hidden_size的\n","        \n","        pack_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n","        out, hid = self.rnn(pack_seq, hid)   # decoder里面的初始state 就是encoder中得到的最后一个字的state\n","        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)   # [batch, y_length, hidden_size] 恢复out的长度，（实际上是把packed_sequence 转回tensor）\n","        _, original_idx = sorted_idx.sort(0, descending=False) #恢复原来的句子顺序\n","        output_seq = unpacked[original_idx.long()].contiguous()  # [batch, y_length, hidden_size] 恢复顺序+深拷贝\n","        \n","        hid = hid[:, original_idx.long()].contiguous()   # [1， batch, hidden_size] decoder的hid_state其实没用，只需要输出即可。\n","        output = F.log_softmax(self.out(output_seq), -1)    \n","        #   self.out 是一个fully connected neural network,输出结果是[batch, y_length, vocab_size] 然后对最后一个维度 vocab_size进行归一化,那么现在output的每一步输出，都是一个vocab_size长度的vector，取其中数值最大的维度就得到了对应的单词\n","        \n","        return output, hid\n"]},{"cell_type":"code","source":["a = nn.Parameter(torch.FloatTensor(1, 4))\n","embed = nn.Embedding(1, 4)\n","print(embed(a))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"nwlqJfZjXB4c","executionInfo":{"status":"error","timestamp":1670183268607,"user_tz":360,"elapsed":313,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"4313c2b0-9d4d-4252-b154-f839bf13ce9b"},"id":"nwlqJfZjXB4c","execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-63-d4c255ec1b84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2197\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2198\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2199\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"]}]},{"cell_type":"code","execution_count":null,"id":"1b424c22","metadata":{"id":"1b424c22"},"outputs":[],"source":["class AttentionDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, dropout = 0.2):\n","        super(AttentionDecoder, self).__init__()\n","        \n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.embed = nn.Embedding(self.vocab_size, self.hidden_size)\n","        self.attn = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.rnn = nn.GRU(self.hidden_size, self.hidden_size, batch_first=True)\n","        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n","        self.V = nn.Parameter(torch.rand(self.hidden_size))\n","    \n","    # def forward(self, input, hidden, encoder_outputs)\n","    def forward(self, y, y_lengths, hid, encoder_out):\n","        # y: [batch_size, y_length] y_length是最长句子的长度（每个句子都被padding成了这么长）\n","        # hid是encoder得到的最后一个字的hidden_state， 实际上包含了整个输入序列的信息\n","\n","        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)    # 依然是句子从长到短排序\n","\n","        y_sorted = y[sorted_idx.long()] # 排序过程    ### y_sorted [batch_size, y_length]\n","        hid = hid[:, sorted_idx.long()] # 排序过程    ### hid [1, batch_size, hidden_size]\n","        \n","        embedded = self.dropout(self.embed(y_sorted)) ### [batch_size, y_length, hidden_size]   \n","        \n","        y_length = len(y_sorted[0])\n","        hid_reshape = hid.transpose(0, 1).repeat(1, y_length, 1)    ### [batch_size, y_length, hidden_size]\n","        \n","        attn_weights = torch.tanh(self.attn(torch.cat((embedded, hid_reshape), 2))) ### [batch_size, y_length, hidden_size]\n","        \n","        ## attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n","        V = self.V.repeat(y_sorted.size(0), 1).unsqueeze(1)\n","        ### hid_reshape = hid.transpose(0, 1)    ### [batch_size, 1, hidden_size]\n","        attn_weights = attn_weights.permute(0, 2, 1)    ### [batch_size, hidden_size, y_length]\n","        attn_weights = torch.bmm(V, attn_weights).squeeze(1)    ### [batch_size, y_length]\n","        attn_weights = F.softmax(attn_weights, dim = 1)\n","        \n","        attn_weights = attn_weights.unsqueeze(2)    ### [batch_size, y_length, 1]\n","        attn_weights = attn_weights.repeat(1, 1, encoder_out.size(dim = 1)) ### [batch_size, y_length, encoder_out_length]\n","        attn_applied = torch.bmm(attn_weights, encoder_out)    ### [batch_size, y_length, hidden_size]\n","\n","        \n","        ## output = torch.cat((embedded[0], attn_applied[0]), 1)\n","        y_sorted_out = torch.cat((embedded, attn_applied), 2)    ### [batch_size, y_length, hidden_size * 2]\n","        \n","        ## output = self.attn_combined(output).unsqueeze(0)\n","        y_sorted_out = self.attn_combine(y_sorted_out)    ### [batch_size, y_length, hidden_size]\n","        \n","        ## output = F.relu(output)\n","        y_sorted_out = F.relu(y_sorted_out)    ### [batch_size, y_length, hidden_size]\n","        \n","        pack_seq = nn.utils.rnn.pack_padded_sequence(y_sorted_out, sorted_len.long().cpu().data.numpy(), batch_first=True)\n","        \n","        ## output, hidden = self.gru(output, hidden)\n","        out, hid = self.rnn(pack_seq, hid)   # decoder里面的初始state 就是encoder中得到的最后一个字的state\n","        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)   # [batch, y_length, hidden_size] 恢复out的长度，（实际上是把packed_sequence 转回tensor）\n","        _, original_idx = sorted_idx.sort(0, descending=False) #恢复原来的句子顺序\n","        output_seq = unpacked[original_idx.long()].contiguous()  # [batch, y_length, hidden_size] 恢复顺序+深拷贝\n","        \n","        ## output = F.log_softmax(self.out(output[0]), dim = 1)\n","        hid = hid[:, original_idx.long()].contiguous()   # [1， batch, hidden_size] decoder的hid_state其实没用，只需要输出即可。\n","        output = F.log_softmax(self.out(output_seq), -1)    \n","        #   self.out 是一个fully connected neural network,输出结果是[batch, y_length, vocab_size] \n","        #   然后对最后一个维度 vocab_size进行归一化,那么现在output的每一步输出，都是一个vocab_size长度的vector，\n","        #  取其中数值最大的维度就得到了对应的单词\n","        \n","        return output, hid\n"]},{"cell_type":"code","source":["class AttentionDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, dropout = 0.2):\n","        super(AttentionDecoder, self).__init__()\n","        self.batch_size = batch_size\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.dropout = nn.Dropout(dropout)\n","        self.embed = nn.Embedding.from_pretrained(pretrained_embedding)\n","        self.fc_hidden = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n","        self.fc_encoder = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n","        self.rnn = nn.GRU(self.hidden_size, self.hidden_size, batch_first=True)\n","        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n","        self.V = nn.Parameter(torch.rand(self.hidden_size))\n","    \n","    # def forward(self, input, hidden, encoder_outputs)\n","    def forward(self, y, y_lengths, hid, encoder_out):\n","\n","        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n","\n","        y_sorted = y[sorted_idx.long()]\n","        hid = hid[:, sorted_idx.long()]\n","        \n","        embedded = self.dropout(self.embed(y_sorted))\n","        \n","        y_length = len(y_sorted[0])\n","\n","        x = torch.tanh(self.fc_hidden(hid.permute(1, 0, 2)) + self.fc_encoder(encoder_out))\n","        V = self.V.repeat(y_sorted.size(0), 1).unsqueeze(1)\n","        alignment_scores = x.bmm(V.permute(0,2,1))\n","        attn_weights = F.softmax(alignment_scores, dim=1)\n","    \n","        context_vector = torch.bmm(attn_weights.permute(0, 2, 1), encoder_out)\n","        \n","        y_sorted_out = torch.cat((embedded, context_vector), 1)\n","        \n","        pack_seq = nn.utils.rnn.pack_padded_sequence(y_sorted_out, sorted_len.long().cpu().data.numpy(), batch_first=True)\n","        \n","        out, hid = self.rnn(pack_seq, hid)\n","        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n","        _, original_idx = sorted_idx.sort(0, descending=False)\n","        output_seq = unpacked[original_idx.long()].contiguous()\n","        \n","        hid = hid[:, original_idx.long()].contiguous()\n","        output = F.log_softmax(self.out(output_seq), -1)    \n","        \n","        return output, hid\n"],"metadata":{"id":"KqyW-rOsaCSv","executionInfo":{"status":"ok","timestamp":1670189994044,"user_tz":360,"elapsed":371,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"id":"KqyW-rOsaCSv","execution_count":147,"outputs":[]},{"cell_type":"code","execution_count":148,"id":"e79a1e73","metadata":{"id":"e79a1e73","executionInfo":{"status":"ok","timestamp":1670189997463,"user_tz":360,"elapsed":2,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["class PlainSeq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(PlainSeq2Seq, self).__init__()\n","        self.encoder = encoder   \n","        self.decoder = decoder\n","        \n","    def forward(self, x, x_lengths, y, y_lengths):\n","        encoder_out, hid = self.encoder(x, x_lengths)  \n","        output, hid = self.decoder(y, y_lengths, hid, encoder_out) \n","        return output, None\n","\n","    def translate(self, x, x_lengths, y, max_length=10): # 这个是进来一个句子进行翻译, y是仅有一个起始字符<go>的decoder输入序列  max_length句子的最大长度\n","        encoder_out, hid = self.encoder(x, x_lengths)   # 将原来的summary进行解码\n","        preds = []\n","        batch_size = x.shape[0]\n","        attns = []\n","        for i in range(max_length):   \n","            output, hid = self.decoder(y, torch.ones(batch_size).long().to(y.device), hid=hid, encoder_out = encoder_out) # 编码，此时decoder的输入就只是<go>\n","            y = output.max(2)[1].view(batch_size, 1) #取dim=2 上的最大值所在的维度，即对应词在vocabulary上的index\n","            preds.append(y)\n","        \n","        return torch.cat(preds, 1), None\n"]},{"cell_type":"code","execution_count":149,"id":"abd791ed","metadata":{"id":"abd791ed","executionInfo":{"status":"ok","timestamp":1670190000998,"user_tz":360,"elapsed":5,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["# masked cross entropy loss\n","class LanguageModelCriterion(nn.Module):\n","    def __init__(self):\n","        super(LanguageModelCriterion, self).__init__()\n","    \n","    def forward(self, input, target, mask):\n","        # input: [batch_size, seq_len, vocab_size]    每个单词的可能性\n","        input = input.contiguous().view(-1, input.size(2))   # [batch_size*seq_len-1, vocab_size] 将input的batch_size和seq_len合并，再计算loss\n","        target = target.contiguous().view(-1, 1)    #  [batch_size*seq_len-1, 1] # target里dim=1存的是正确summary 的index\n","        \n","        mask = mask.contiguous().view(-1, 1)   # [batch_size*seq_len-1, 1]\n","        output = -input.gather(1, target) * mask # 在每个vocab_size维度取正确单词的索引， 但是里面有很多是填充进去的， 所以mask去掉这些填充的， \n","        output = torch.sum(output) / torch.sum(mask)\n","        \n","        return output  # [batch_size*seq_len-1, 1]\n"]},{"cell_type":"code","execution_count":150,"id":"7e557981","metadata":{"id":"7e557981","executionInfo":{"status":"ok","timestamp":1670190005347,"user_tz":360,"elapsed":1426,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","dropout = 0.2\n","hidden_size = 300\n","encoder = PlainEncoder(vocab_size=len(word_embedding_matrix), hidden_size=hidden_size, dropout=dropout)\n","decoder = AttentionDecoder2(vocab_size=len(word_embedding_matrix), hidden_size=hidden_size, dropout=dropout)\n","\n","model = PlainSeq2Seq(encoder, decoder)\n","model = model.to(device)\n","loss_fn = LanguageModelCriterion().to(device)\n","optimizer = torch.optim.Adam(model.parameters())\n"]},{"cell_type":"code","execution_count":151,"id":"a50d0b48","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"a50d0b48","executionInfo":{"status":"ok","timestamp":1670193592297,"user_tz":360,"elapsed":3584562,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"546f7cf6-d014-48b1-aa60-7aa0729d6374"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 iteration 0 loss 11.020692825317383\n","Epoch 0 iteration 100 loss 5.71868896484375\n","Epoch 0 iteration 200 loss 5.69000244140625\n","Epoch 0 iteration 300 loss 5.816007137298584\n","Epoch 0 iteration 400 loss 5.469925880432129\n","Epoch 0 iteration 500 loss 5.314994812011719\n","Epoch 0 iteration 600 loss 5.097631454467773\n","Epoch 0 iteration 700 loss 5.10923957824707\n","Epoch 0 iteration 800 loss 5.235311031341553\n","Epoch 0 iteration 900 loss 5.380863666534424\n","Epoch 0 iteration 1000 loss 4.476003170013428\n","Epoch 0 iteration 1100 loss 5.350000858306885\n","Epoch 0 iteration 1200 loss 4.783313274383545\n","Epoch 0 iteration 1300 loss 4.949572563171387\n","Epoch 0 iteration 1400 loss 4.8847575187683105\n","Epoch 0 iteration 1500 loss 4.7320332527160645\n","Epoch 0 iteration 1600 loss 4.600372791290283\n","Epoch 0 iteration 1700 loss 4.786974906921387\n","Epoch 0 iteration 1800 loss 4.534893989562988\n","Epoch 0 iteration 1900 loss 4.374423503875732\n","Epoch 0 iteration 2000 loss 4.390428066253662\n","Epoch 0 iteration 2100 loss 4.618144512176514\n","Epoch 0 iteration 2200 loss 4.413948059082031\n","Epoch 0 iteration 2300 loss 4.403323173522949\n","Epoch 0 iteration 2400 loss 4.474734306335449\n","Epoch 0 iteration 2500 loss 4.428361415863037\n","Epoch 0 iteration 2600 loss 4.121217727661133\n","Epoch 0 iteration 2700 loss 4.38892126083374\n","Epoch 0 iteration 2800 loss 4.309462547302246\n","Epoch 0 iteration 2900 loss 3.7594292163848877\n","Epoch 0 iteration 3000 loss 4.434816360473633\n","Epoch 0 iteration 3100 loss 4.0162672996521\n","Epoch 0 iteration 3200 loss 4.602933406829834\n","Epoch 0 iteration 3300 loss 4.135194778442383\n","Epoch 0 iteration 3400 loss 3.785842180252075\n","Epoch 0 iteration 3500 loss 4.14091157913208\n","Epoch 0 iteration 3600 loss 4.127406120300293\n","Epoch 0 iteration 3700 loss 3.8106393814086914\n","Epoch 0 iteration 3800 loss 4.345627307891846\n","Epoch 0 Training loss 4.718031086582317\n","Evaluation loss 4.055204111688386\n","Epoch 1 iteration 0 loss 4.579339981079102\n","Epoch 1 iteration 100 loss 3.768225908279419\n","Epoch 1 iteration 200 loss 3.860851764678955\n","Epoch 1 iteration 300 loss 4.05866003036499\n","Epoch 1 iteration 400 loss 3.8381619453430176\n","Epoch 1 iteration 500 loss 4.022366046905518\n","Epoch 1 iteration 600 loss 3.962899923324585\n","Epoch 1 iteration 700 loss 3.9673638343811035\n","Epoch 1 iteration 800 loss 4.312187194824219\n","Epoch 1 iteration 900 loss 4.091095447540283\n","Epoch 1 iteration 1000 loss 3.5038163661956787\n","Epoch 1 iteration 1100 loss 4.37804651260376\n","Epoch 1 iteration 1200 loss 3.734546661376953\n","Epoch 1 iteration 1300 loss 4.077592849731445\n","Epoch 1 iteration 1400 loss 3.847722291946411\n","Epoch 1 iteration 1500 loss 3.9180009365081787\n","Epoch 1 iteration 1600 loss 3.6643788814544678\n","Epoch 1 iteration 1700 loss 3.912719964981079\n","Epoch 1 iteration 1800 loss 3.739215850830078\n","Epoch 1 iteration 1900 loss 3.514760732650757\n","Epoch 1 iteration 2000 loss 3.6188817024230957\n","Epoch 1 iteration 2100 loss 3.8216845989227295\n","Epoch 1 iteration 2200 loss 3.7193052768707275\n","Epoch 1 iteration 2300 loss 3.709728240966797\n","Epoch 1 iteration 2400 loss 3.6099023818969727\n","Epoch 1 iteration 2500 loss 3.6952719688415527\n","Epoch 1 iteration 2600 loss 3.5271096229553223\n","Epoch 1 iteration 2700 loss 3.8042306900024414\n","Epoch 1 iteration 2800 loss 3.702193260192871\n","Epoch 1 iteration 2900 loss 3.178128480911255\n","Epoch 1 iteration 3000 loss 3.782931089401245\n","Epoch 1 iteration 3100 loss 3.318082094192505\n","Epoch 1 iteration 3200 loss 3.9501543045043945\n","Epoch 1 iteration 3300 loss 3.6154978275299072\n","Epoch 1 iteration 3400 loss 3.243553400039673\n","Epoch 1 iteration 3500 loss 3.5220577716827393\n","Epoch 1 iteration 3600 loss 3.630434274673462\n","Epoch 1 iteration 3700 loss 3.2672030925750732\n","Epoch 1 iteration 3800 loss 3.7862982749938965\n","Epoch 1 Training loss 3.787529356008772\n","Epoch 2 iteration 0 loss 4.083586692810059\n","Epoch 2 iteration 100 loss 3.3919830322265625\n","Epoch 2 iteration 200 loss 3.298373222351074\n","Epoch 2 iteration 300 loss 3.5011355876922607\n","Epoch 2 iteration 400 loss 3.430797576904297\n","Epoch 2 iteration 500 loss 3.607550859451294\n","Epoch 2 iteration 600 loss 3.5653231143951416\n","Epoch 2 iteration 700 loss 3.5993387699127197\n","Epoch 2 iteration 800 loss 3.922283411026001\n","Epoch 2 iteration 900 loss 3.575917959213257\n","Epoch 2 iteration 1000 loss 3.140455484390259\n","Epoch 2 iteration 1100 loss 3.876042366027832\n","Epoch 2 iteration 1200 loss 3.3198487758636475\n","Epoch 2 iteration 1300 loss 3.7168214321136475\n","Epoch 2 iteration 1400 loss 3.389044761657715\n","Epoch 2 iteration 1500 loss 3.504570960998535\n","Epoch 2 iteration 1600 loss 3.2549564838409424\n","Epoch 2 iteration 1700 loss 3.4115848541259766\n","Epoch 2 iteration 1800 loss 3.305271863937378\n","Epoch 2 iteration 1900 loss 3.1878960132598877\n","Epoch 2 iteration 2000 loss 3.2298460006713867\n","Epoch 2 iteration 2100 loss 3.441054344177246\n","Epoch 2 iteration 2200 loss 3.3400986194610596\n","Epoch 2 iteration 2300 loss 3.3934166431427\n","Epoch 2 iteration 2400 loss 3.2897777557373047\n","Epoch 2 iteration 2500 loss 3.369917154312134\n","Epoch 2 iteration 2600 loss 3.160003662109375\n","Epoch 2 iteration 2700 loss 3.415785312652588\n","Epoch 2 iteration 2800 loss 3.3936948776245117\n","Epoch 2 iteration 2900 loss 2.8455960750579834\n","Epoch 2 iteration 3000 loss 3.4506514072418213\n","Epoch 2 iteration 3100 loss 2.9267101287841797\n","Epoch 2 iteration 3200 loss 3.5545334815979004\n","Epoch 2 iteration 3300 loss 3.267049789428711\n","Epoch 2 iteration 3400 loss 2.920328378677368\n","Epoch 2 iteration 3500 loss 3.145111322402954\n","Epoch 2 iteration 3600 loss 3.2838284969329834\n","Epoch 2 iteration 3700 loss 3.0110085010528564\n","Epoch 2 iteration 3800 loss 3.4865167140960693\n","Epoch 2 Training loss 3.3976277331244336\n","Epoch 3 iteration 0 loss 3.77840518951416\n","Epoch 3 iteration 100 loss 3.0487258434295654\n","Epoch 3 iteration 200 loss 3.0244650840759277\n","Epoch 3 iteration 300 loss 3.169914960861206\n","Epoch 3 iteration 400 loss 3.2350881099700928\n","Epoch 3 iteration 500 loss 3.4464118480682373\n","Epoch 3 iteration 600 loss 3.394993543624878\n","Epoch 3 iteration 700 loss 3.4133691787719727\n","Epoch 3 iteration 800 loss 3.6120243072509766\n","Epoch 3 iteration 900 loss 3.306487560272217\n","Epoch 3 iteration 1000 loss 2.9165849685668945\n","Epoch 3 iteration 1100 loss 3.578932046890259\n","Epoch 3 iteration 1200 loss 3.095974922180176\n","Epoch 3 iteration 1300 loss 3.513716697692871\n","Epoch 3 iteration 1400 loss 3.0933990478515625\n","Epoch 3 iteration 1500 loss 3.27048659324646\n","Epoch 3 iteration 1600 loss 3.0425243377685547\n","Epoch 3 iteration 1700 loss 3.1709961891174316\n","Epoch 3 iteration 1800 loss 3.039355993270874\n","Epoch 3 iteration 1900 loss 2.974810838699341\n","Epoch 3 iteration 2000 loss 2.9529225826263428\n","Epoch 3 iteration 2100 loss 3.1944758892059326\n","Epoch 3 iteration 2200 loss 3.077186346054077\n","Epoch 3 iteration 2300 loss 3.1344447135925293\n","Epoch 3 iteration 2400 loss 2.9468443393707275\n","Epoch 3 iteration 2500 loss 3.1648354530334473\n","Epoch 3 iteration 2600 loss 2.8757686614990234\n","Epoch 3 iteration 2700 loss 3.1797590255737305\n","Epoch 3 iteration 2800 loss 3.1706621646881104\n","Epoch 3 iteration 2900 loss 2.694897174835205\n","Epoch 3 iteration 3000 loss 3.19954514503479\n","Epoch 3 iteration 3100 loss 2.6911113262176514\n","Epoch 3 iteration 3200 loss 3.326395273208618\n","Epoch 3 iteration 3300 loss 3.084534168243408\n","Epoch 3 iteration 3400 loss 2.761448621749878\n","Epoch 3 iteration 3500 loss 2.9321491718292236\n","Epoch 3 iteration 3600 loss 3.086534261703491\n","Epoch 3 iteration 3700 loss 2.7684402465820312\n","Epoch 3 iteration 3800 loss 3.2486572265625\n","Epoch 3 Training loss 3.1480570822453102\n","Epoch 4 iteration 0 loss 3.5099985599517822\n","Epoch 4 iteration 100 loss 2.9062912464141846\n","Epoch 4 iteration 200 loss 2.7824037075042725\n","Epoch 4 iteration 300 loss 2.8770382404327393\n","Epoch 4 iteration 400 loss 3.0849416255950928\n","Epoch 4 iteration 500 loss 3.2955756187438965\n","Epoch 4 iteration 600 loss 3.194143533706665\n","Epoch 4 iteration 700 loss 3.2353131771087646\n","Epoch 4 iteration 800 loss 3.3722574710845947\n","Epoch 4 iteration 900 loss 3.1295266151428223\n","Epoch 4 iteration 1000 loss 2.75390625\n","Epoch 4 iteration 1100 loss 3.3494672775268555\n","Epoch 4 iteration 1200 loss 2.887834072113037\n","Epoch 4 iteration 1300 loss 3.2651898860931396\n","Epoch 4 iteration 1400 loss 2.9003541469573975\n","Epoch 4 iteration 1500 loss 3.121501922607422\n","Epoch 4 iteration 1600 loss 2.8078408241271973\n","Epoch 4 iteration 1700 loss 2.929765224456787\n","Epoch 4 iteration 1800 loss 2.910356044769287\n","Epoch 4 iteration 1900 loss 2.9078528881073\n","Epoch 4 iteration 2000 loss 2.802605152130127\n","Epoch 4 iteration 2100 loss 3.0516700744628906\n","Epoch 4 iteration 2200 loss 2.8780789375305176\n","Epoch 4 iteration 2300 loss 3.0199344158172607\n","Epoch 4 iteration 2400 loss 2.7183666229248047\n","Epoch 4 iteration 2500 loss 2.9723281860351562\n","Epoch 4 iteration 2600 loss 2.73664927482605\n","Epoch 4 iteration 2700 loss 2.9312453269958496\n","Epoch 4 iteration 2800 loss 3.0672521591186523\n","Epoch 4 iteration 2900 loss 2.512803316116333\n","Epoch 4 iteration 3000 loss 2.9745965003967285\n","Epoch 4 iteration 3100 loss 2.4456663131713867\n","Epoch 4 iteration 3200 loss 3.083503246307373\n","Epoch 4 iteration 3300 loss 2.9536681175231934\n","Epoch 4 iteration 3400 loss 2.5716922283172607\n","Epoch 4 iteration 3500 loss 2.707470178604126\n","Epoch 4 iteration 3600 loss 2.8302903175354004\n","Epoch 4 iteration 3700 loss 2.621817111968994\n","Epoch 4 iteration 3800 loss 3.147174835205078\n","Epoch 4 Training loss 2.9691248114532276\n","Epoch 5 iteration 0 loss 3.354342222213745\n","Epoch 5 iteration 100 loss 2.786268949508667\n","Epoch 5 iteration 200 loss 2.624833345413208\n","Epoch 5 iteration 300 loss 2.780482292175293\n","Epoch 5 iteration 400 loss 2.920621156692505\n","Epoch 5 iteration 500 loss 3.1399905681610107\n","Epoch 5 iteration 600 loss 3.068561315536499\n","Epoch 5 iteration 700 loss 3.093142032623291\n","Epoch 5 iteration 800 loss 3.244941473007202\n","Epoch 5 iteration 900 loss 3.0069618225097656\n","Epoch 5 iteration 1000 loss 2.6121630668640137\n","Epoch 5 iteration 1100 loss 3.192633867263794\n","Epoch 5 iteration 1200 loss 2.7283332347869873\n","Epoch 5 iteration 1300 loss 3.165231466293335\n","Epoch 5 iteration 1400 loss 2.738858461380005\n","Epoch 5 iteration 1500 loss 2.9033124446868896\n","Epoch 5 iteration 1600 loss 2.7428884506225586\n","Epoch 5 iteration 1700 loss 2.7083818912506104\n","Epoch 5 iteration 1800 loss 2.7689719200134277\n","Epoch 5 iteration 1900 loss 2.7088623046875\n","Epoch 5 iteration 2000 loss 2.6607844829559326\n","Epoch 5 iteration 2100 loss 2.8979616165161133\n","Epoch 5 iteration 2200 loss 2.757279634475708\n","Epoch 5 iteration 2300 loss 2.888819456100464\n","Epoch 5 iteration 2400 loss 2.6701202392578125\n","Epoch 5 iteration 2500 loss 2.8723983764648438\n","Epoch 5 iteration 2600 loss 2.586712598800659\n","Epoch 5 iteration 2700 loss 2.7917494773864746\n","Epoch 5 iteration 2800 loss 2.895010471343994\n","Epoch 5 iteration 2900 loss 2.4098148345947266\n","Epoch 5 iteration 3000 loss 2.8520820140838623\n","Epoch 5 iteration 3100 loss 2.354964256286621\n","Epoch 5 iteration 3200 loss 3.0126967430114746\n","Epoch 5 iteration 3300 loss 2.7092933654785156\n","Epoch 5 iteration 3400 loss 2.5415894985198975\n","Epoch 5 iteration 3500 loss 2.585540294647217\n","Epoch 5 iteration 3600 loss 2.753939628601074\n","Epoch 5 iteration 3700 loss 2.5643129348754883\n","Epoch 5 iteration 3800 loss 3.0349268913269043\n","Epoch 5 Training loss 2.836188894620733\n","Evaluation loss 3.2302574998581037\n","Epoch 6 iteration 0 loss 3.1588034629821777\n","Epoch 6 iteration 100 loss 2.687892198562622\n","Epoch 6 iteration 200 loss 2.5310511589050293\n","Epoch 6 iteration 300 loss 2.630863904953003\n","Epoch 6 iteration 400 loss 2.763538360595703\n","Epoch 6 iteration 500 loss 3.0373833179473877\n","Epoch 6 iteration 600 loss 2.938802719116211\n","Epoch 6 iteration 700 loss 2.9594268798828125\n","Epoch 6 iteration 800 loss 3.1610774993896484\n","Epoch 6 iteration 900 loss 2.8669214248657227\n","Epoch 6 iteration 1000 loss 2.5037567615509033\n","Epoch 6 iteration 1100 loss 3.0212044715881348\n","Epoch 6 iteration 1200 loss 2.5836777687072754\n","Epoch 6 iteration 1300 loss 3.0400772094726562\n","Epoch 6 iteration 1400 loss 2.6748762130737305\n","Epoch 6 iteration 1500 loss 2.9025824069976807\n","Epoch 6 iteration 1600 loss 2.636622905731201\n","Epoch 6 iteration 1700 loss 2.606046438217163\n","Epoch 6 iteration 1800 loss 2.7037134170532227\n","Epoch 6 iteration 1900 loss 2.664323091506958\n","Epoch 6 iteration 2000 loss 2.5064542293548584\n","Epoch 6 iteration 2100 loss 2.8191347122192383\n","Epoch 6 iteration 2200 loss 2.595810890197754\n","Epoch 6 iteration 2300 loss 2.7601778507232666\n","Epoch 6 iteration 2400 loss 2.5700368881225586\n","Epoch 6 iteration 2500 loss 2.778630256652832\n","Epoch 6 iteration 2600 loss 2.5162811279296875\n","Epoch 6 iteration 2700 loss 2.610466718673706\n","Epoch 6 iteration 2800 loss 2.811478614807129\n","Epoch 6 iteration 2900 loss 2.3157339096069336\n","Epoch 6 iteration 3000 loss 2.714238166809082\n","Epoch 6 iteration 3100 loss 2.2431554794311523\n","Epoch 6 iteration 3200 loss 2.887225389480591\n","Epoch 6 iteration 3300 loss 2.763035535812378\n","Epoch 6 iteration 3400 loss 2.3891849517822266\n","Epoch 6 iteration 3500 loss 2.4559524059295654\n","Epoch 6 iteration 3600 loss 2.6316046714782715\n","Epoch 6 iteration 3700 loss 2.455451011657715\n","Epoch 6 iteration 3800 loss 2.915058135986328\n","Epoch 6 Training loss 2.733316573307806\n","Epoch 7 iteration 0 loss 3.082360029220581\n","Epoch 7 iteration 100 loss 2.602097272872925\n","Epoch 7 iteration 200 loss 2.452296018600464\n","Epoch 7 iteration 300 loss 2.5151426792144775\n","Epoch 7 iteration 400 loss 2.686624765396118\n","Epoch 7 iteration 500 loss 2.9813272953033447\n","Epoch 7 iteration 600 loss 2.9328737258911133\n","Epoch 7 iteration 700 loss 2.9730384349823\n","Epoch 7 iteration 800 loss 3.0113329887390137\n","Epoch 7 iteration 900 loss 2.8086166381835938\n","Epoch 7 iteration 1000 loss 2.3784728050231934\n","Epoch 7 iteration 1100 loss 2.9468703269958496\n","Epoch 7 iteration 1200 loss 2.4921250343322754\n","Epoch 7 iteration 1300 loss 2.951195001602173\n","Epoch 7 iteration 1400 loss 2.5867881774902344\n","Epoch 7 iteration 1500 loss 2.6983797550201416\n","Epoch 7 iteration 1600 loss 2.508582353591919\n","Epoch 7 iteration 1700 loss 2.568507432937622\n","Epoch 7 iteration 1800 loss 2.6573712825775146\n","Epoch 7 iteration 1900 loss 2.604804754257202\n","Epoch 7 iteration 2000 loss 2.4455604553222656\n","Epoch 7 iteration 2100 loss 2.7265846729278564\n","Epoch 7 iteration 2200 loss 2.5179240703582764\n","Epoch 7 iteration 2300 loss 2.72481369972229\n","Epoch 7 iteration 2400 loss 2.4918692111968994\n","Epoch 7 iteration 2500 loss 2.697519540786743\n","Epoch 7 iteration 2600 loss 2.421771287918091\n","Epoch 7 iteration 2700 loss 2.505824089050293\n","Epoch 7 iteration 2800 loss 2.7591335773468018\n","Epoch 7 iteration 2900 loss 2.243849515914917\n","Epoch 7 iteration 3000 loss 2.553650140762329\n","Epoch 7 iteration 3100 loss 2.187680959701538\n","Epoch 7 iteration 3200 loss 2.747640609741211\n","Epoch 7 iteration 3300 loss 2.6386680603027344\n","Epoch 7 iteration 3400 loss 2.356131076812744\n","Epoch 7 iteration 3500 loss 2.3839101791381836\n","Epoch 7 iteration 3600 loss 2.5317771434783936\n","Epoch 7 iteration 3700 loss 2.425640344619751\n","Epoch 7 iteration 3800 loss 2.8752570152282715\n","Epoch 7 Training loss 2.6506721627368037\n","Epoch 8 iteration 0 loss 2.956946611404419\n","Epoch 8 iteration 100 loss 2.502168655395508\n","Epoch 8 iteration 200 loss 2.3483824729919434\n","Epoch 8 iteration 300 loss 2.3952486515045166\n","Epoch 8 iteration 400 loss 2.609987258911133\n","Epoch 8 iteration 500 loss 2.9349570274353027\n","Epoch 8 iteration 600 loss 2.813957929611206\n","Epoch 8 iteration 700 loss 2.8476998805999756\n","Epoch 8 iteration 800 loss 3.004159927368164\n","Epoch 8 iteration 900 loss 2.79225754737854\n","Epoch 8 iteration 1000 loss 2.345703601837158\n","Epoch 8 iteration 1100 loss 2.8576273918151855\n","Epoch 8 iteration 1200 loss 2.4970173835754395\n","Epoch 8 iteration 1300 loss 2.811167001724243\n","Epoch 8 iteration 1400 loss 2.494866371154785\n","Epoch 8 iteration 1500 loss 2.710282325744629\n","Epoch 8 iteration 1600 loss 2.4637250900268555\n","Epoch 8 iteration 1700 loss 2.446957588195801\n","Epoch 8 iteration 1800 loss 2.557579517364502\n","Epoch 8 iteration 1900 loss 2.520526170730591\n","Epoch 8 iteration 2000 loss 2.402557373046875\n","Epoch 8 iteration 2100 loss 2.703763246536255\n","Epoch 8 iteration 2200 loss 2.4629249572753906\n","Epoch 8 iteration 2300 loss 2.6360111236572266\n","Epoch 8 iteration 2400 loss 2.3378963470458984\n","Epoch 8 iteration 2500 loss 2.621389150619507\n","Epoch 8 iteration 2600 loss 2.360611915588379\n","Epoch 8 iteration 2700 loss 2.4054338932037354\n","Epoch 8 iteration 2800 loss 2.6622071266174316\n","Epoch 8 iteration 2900 loss 2.177504301071167\n","Epoch 8 iteration 3000 loss 2.5197794437408447\n","Epoch 8 iteration 3100 loss 2.1105782985687256\n","Epoch 8 iteration 3200 loss 2.733217477798462\n","Epoch 8 iteration 3300 loss 2.5009117126464844\n","Epoch 8 iteration 3400 loss 2.3094863891601562\n","Epoch 8 iteration 3500 loss 2.2882134914398193\n","Epoch 8 iteration 3600 loss 2.467242956161499\n","Epoch 8 iteration 3700 loss 2.3174307346343994\n","Epoch 8 iteration 3800 loss 2.7976276874542236\n","Epoch 8 Training loss 2.584793496737454\n","Epoch 9 iteration 0 loss 2.88702392578125\n","Epoch 9 iteration 100 loss 2.4744105339050293\n","Epoch 9 iteration 200 loss 2.2875142097473145\n","Epoch 9 iteration 300 loss 2.3063600063323975\n","Epoch 9 iteration 400 loss 2.5580713748931885\n","Epoch 9 iteration 500 loss 2.8414790630340576\n","Epoch 9 iteration 600 loss 2.7574715614318848\n","Epoch 9 iteration 700 loss 2.8382787704467773\n","Epoch 9 iteration 800 loss 2.899136543273926\n","Epoch 9 iteration 900 loss 2.7105820178985596\n","Epoch 9 iteration 1000 loss 2.353085994720459\n","Epoch 9 iteration 1100 loss 2.7620086669921875\n","Epoch 9 iteration 1200 loss 2.3549959659576416\n","Epoch 9 iteration 1300 loss 2.799093723297119\n","Epoch 9 iteration 1400 loss 2.494893789291382\n","Epoch 9 iteration 1500 loss 2.68392276763916\n","Epoch 9 iteration 1600 loss 2.377016067504883\n","Epoch 9 iteration 1700 loss 2.4077792167663574\n","Epoch 9 iteration 1800 loss 2.493337392807007\n","Epoch 9 iteration 1900 loss 2.3938186168670654\n","Epoch 9 iteration 2000 loss 2.2873380184173584\n","Epoch 9 iteration 2100 loss 2.610074043273926\n","Epoch 9 iteration 2200 loss 2.384634017944336\n","Epoch 9 iteration 2300 loss 2.5851588249206543\n","Epoch 9 iteration 2400 loss 2.3498024940490723\n","Epoch 9 iteration 2500 loss 2.544525623321533\n","Epoch 9 iteration 2600 loss 2.2844653129577637\n","Epoch 9 iteration 2700 loss 2.3062050342559814\n","Epoch 9 iteration 2800 loss 2.6505839824676514\n","Epoch 9 iteration 2900 loss 2.094362258911133\n","Epoch 9 iteration 3000 loss 2.446442127227783\n","Epoch 9 iteration 3100 loss 2.043367624282837\n","Epoch 9 iteration 3200 loss 2.6501998901367188\n","Epoch 9 iteration 3300 loss 2.464291572570801\n","Epoch 9 iteration 3400 loss 2.2353992462158203\n","Epoch 9 iteration 3500 loss 2.249290943145752\n","Epoch 9 iteration 3600 loss 2.4568092823028564\n","Epoch 9 iteration 3700 loss 2.4080824851989746\n","Epoch 9 iteration 3800 loss 2.73356032371521\n","Epoch 9 Training loss 2.5296402043852786\n","Epoch 10 iteration 0 loss 2.8081088066101074\n","Epoch 10 iteration 100 loss 2.377378463745117\n","Epoch 10 iteration 200 loss 2.2773163318634033\n","Epoch 10 iteration 300 loss 2.207202672958374\n","Epoch 10 iteration 400 loss 2.534496784210205\n","Epoch 10 iteration 500 loss 2.8770751953125\n","Epoch 10 iteration 600 loss 2.672708511352539\n","Epoch 10 iteration 700 loss 2.7218220233917236\n","Epoch 10 iteration 800 loss 2.876570701599121\n","Epoch 10 iteration 900 loss 2.6717982292175293\n","Epoch 10 iteration 1000 loss 2.3004486560821533\n","Epoch 10 iteration 1100 loss 2.7410643100738525\n","Epoch 10 iteration 1200 loss 2.3700079917907715\n","Epoch 10 iteration 1300 loss 2.7576937675476074\n","Epoch 10 iteration 1400 loss 2.451319456100464\n","Epoch 10 iteration 1500 loss 2.6389026641845703\n","Epoch 10 iteration 1600 loss 2.2740607261657715\n","Epoch 10 iteration 1700 loss 2.4276034832000732\n","Epoch 10 iteration 1800 loss 2.4067893028259277\n","Epoch 10 iteration 1900 loss 2.381930112838745\n","Epoch 10 iteration 2000 loss 2.30541729927063\n","Epoch 10 iteration 2100 loss 2.575737953186035\n","Epoch 10 iteration 2200 loss 2.321559190750122\n","Epoch 10 iteration 2300 loss 2.5551905632019043\n","Epoch 10 iteration 2400 loss 2.28840708732605\n","Epoch 10 iteration 2500 loss 2.4873898029327393\n","Epoch 10 iteration 2600 loss 2.249880790710449\n","Epoch 10 iteration 2700 loss 2.330843925476074\n","Epoch 10 iteration 2800 loss 2.6331286430358887\n","Epoch 10 iteration 2900 loss 2.0907232761383057\n","Epoch 10 iteration 3000 loss 2.3544633388519287\n","Epoch 10 iteration 3100 loss 1.9818354845046997\n","Epoch 10 iteration 3200 loss 2.560452461242676\n","Epoch 10 iteration 3300 loss 2.437318801879883\n","Epoch 10 iteration 3400 loss 2.274367570877075\n","Epoch 10 iteration 3500 loss 2.1592981815338135\n","Epoch 10 iteration 3600 loss 2.4331743717193604\n","Epoch 10 iteration 3700 loss 2.2827987670898438\n","Epoch 10 iteration 3800 loss 2.6596767902374268\n","Epoch 10 Training loss 2.484235731914617\n","Evaluation loss 3.16692771173642\n","Epoch 11 iteration 0 loss 2.835993528366089\n","Epoch 11 iteration 100 loss 2.4164350032806396\n","Epoch 11 iteration 200 loss 2.3028712272644043\n","Epoch 11 iteration 300 loss 2.219193935394287\n","Epoch 11 iteration 400 loss 2.5430619716644287\n","Epoch 11 iteration 500 loss 2.7285234928131104\n","Epoch 11 iteration 600 loss 2.6872193813323975\n","Epoch 11 iteration 700 loss 2.6923844814300537\n","Epoch 11 iteration 800 loss 2.7513110637664795\n","Epoch 11 iteration 900 loss 2.651216983795166\n","Epoch 11 iteration 1000 loss 2.2452633380889893\n","Epoch 11 iteration 1100 loss 2.658665418624878\n","Epoch 11 iteration 1200 loss 2.2320401668548584\n","Epoch 11 iteration 1300 loss 2.6920580863952637\n","Epoch 11 iteration 1400 loss 2.369403839111328\n","Epoch 11 iteration 1500 loss 2.5696866512298584\n","Epoch 11 iteration 1600 loss 2.286231756210327\n","Epoch 11 iteration 1700 loss 2.372014045715332\n","Epoch 11 iteration 1800 loss 2.3889565467834473\n","Epoch 11 iteration 1900 loss 2.3516130447387695\n","Epoch 11 iteration 2000 loss 2.1822028160095215\n","Epoch 11 iteration 2100 loss 2.548485517501831\n","Epoch 11 iteration 2200 loss 2.2397279739379883\n","Epoch 11 iteration 2300 loss 2.562424659729004\n","Epoch 11 iteration 2400 loss 2.250457763671875\n","Epoch 11 iteration 2500 loss 2.488463878631592\n","Epoch 11 iteration 2600 loss 2.163492202758789\n","Epoch 11 iteration 2700 loss 2.21572208404541\n","Epoch 11 iteration 2800 loss 2.540954828262329\n","Epoch 11 iteration 2900 loss 2.0752251148223877\n","Epoch 11 iteration 3000 loss 2.359980583190918\n","Epoch 11 iteration 3100 loss 2.0222604274749756\n","Epoch 11 iteration 3200 loss 2.501054286956787\n","Epoch 11 iteration 3300 loss 2.4434235095977783\n","Epoch 11 iteration 3400 loss 2.2195632457733154\n","Epoch 11 iteration 3500 loss 2.180612802505493\n","Epoch 11 iteration 3600 loss 2.366748571395874\n","Epoch 11 iteration 3700 loss 2.303983688354492\n","Epoch 11 iteration 3800 loss 2.6319684982299805\n","Epoch 11 Training loss 2.4455514788096733\n","Epoch 12 iteration 0 loss 2.81355619430542\n","Epoch 12 iteration 100 loss 2.3615219593048096\n","Epoch 12 iteration 200 loss 2.2108097076416016\n","Epoch 12 iteration 300 loss 2.1214916706085205\n","Epoch 12 iteration 400 loss 2.494699239730835\n","Epoch 12 iteration 500 loss 2.735369920730591\n","Epoch 12 iteration 600 loss 2.681171178817749\n","Epoch 12 iteration 700 loss 2.648880958557129\n","Epoch 12 iteration 800 loss 2.875396728515625\n","Epoch 12 iteration 900 loss 2.6047110557556152\n","Epoch 12 iteration 1000 loss 2.252066135406494\n","Epoch 12 iteration 1100 loss 2.6058459281921387\n","Epoch 12 iteration 1200 loss 2.2516889572143555\n","Epoch 12 iteration 1300 loss 2.7278523445129395\n","Epoch 12 iteration 1400 loss 2.3774495124816895\n","Epoch 12 iteration 1500 loss 2.5101232528686523\n","Epoch 12 iteration 1600 loss 2.220263719558716\n","Epoch 12 iteration 1700 loss 2.3552048206329346\n","Epoch 12 iteration 1800 loss 2.387362241744995\n","Epoch 12 iteration 1900 loss 2.3400840759277344\n","Epoch 12 iteration 2000 loss 2.1684374809265137\n","Epoch 12 iteration 2100 loss 2.509753704071045\n","Epoch 12 iteration 2200 loss 2.1723763942718506\n","Epoch 12 iteration 2300 loss 2.5033822059631348\n","Epoch 12 iteration 2400 loss 2.2360572814941406\n","Epoch 12 iteration 2500 loss 2.4491188526153564\n","Epoch 12 iteration 2600 loss 2.175755500793457\n","Epoch 12 iteration 2700 loss 2.1583635807037354\n","Epoch 12 iteration 2800 loss 2.471665143966675\n","Epoch 12 iteration 2900 loss 2.0383691787719727\n","Epoch 12 iteration 3000 loss 2.2806599140167236\n","Epoch 12 iteration 3100 loss 1.907034993171692\n","Epoch 12 iteration 3200 loss 2.564161539077759\n","Epoch 12 iteration 3300 loss 2.321293354034424\n","Epoch 12 iteration 3400 loss 2.1866133213043213\n","Epoch 12 iteration 3500 loss 2.088127374649048\n","Epoch 12 iteration 3600 loss 2.3293282985687256\n","Epoch 12 iteration 3700 loss 2.233765125274658\n","Epoch 12 iteration 3800 loss 2.678654193878174\n","Epoch 12 Training loss 2.4124438044007745\n","Epoch 13 iteration 0 loss 2.6999807357788086\n","Epoch 13 iteration 100 loss 2.356416940689087\n","Epoch 13 iteration 200 loss 2.2495362758636475\n","Epoch 13 iteration 300 loss 2.0886149406433105\n","Epoch 13 iteration 400 loss 2.4899706840515137\n","Epoch 13 iteration 500 loss 2.648503541946411\n","Epoch 13 iteration 600 loss 2.6031994819641113\n","Epoch 13 iteration 700 loss 2.5862183570861816\n","Epoch 13 iteration 800 loss 2.7813332080841064\n","Epoch 13 iteration 900 loss 2.5884804725646973\n","Epoch 13 iteration 1000 loss 2.143911600112915\n","Epoch 13 iteration 1100 loss 2.597581386566162\n","Epoch 13 iteration 1200 loss 2.246497392654419\n","Epoch 13 iteration 1300 loss 2.6605381965637207\n","Epoch 13 iteration 1400 loss 2.3471357822418213\n","Epoch 13 iteration 1500 loss 2.513939619064331\n","Epoch 13 iteration 1600 loss 2.145888328552246\n","Epoch 13 iteration 1700 loss 2.285144805908203\n","Epoch 13 iteration 1800 loss 2.2659780979156494\n","Epoch 13 iteration 1900 loss 2.324233055114746\n","Epoch 13 iteration 2000 loss 2.131528377532959\n","Epoch 13 iteration 2100 loss 2.4140501022338867\n","Epoch 13 iteration 2200 loss 2.1347146034240723\n","Epoch 13 iteration 2300 loss 2.5412702560424805\n","Epoch 13 iteration 2400 loss 2.1670713424682617\n","Epoch 13 iteration 2500 loss 2.3691458702087402\n","Epoch 13 iteration 2600 loss 2.112417459487915\n","Epoch 13 iteration 2700 loss 2.1164252758026123\n","Epoch 13 iteration 2800 loss 2.3975555896759033\n","Epoch 13 iteration 2900 loss 2.0323925018310547\n","Epoch 13 iteration 3000 loss 2.290191173553467\n","Epoch 13 iteration 3100 loss 1.9684642553329468\n","Epoch 13 iteration 3200 loss 2.5010175704956055\n","Epoch 13 iteration 3300 loss 2.3279645442962646\n","Epoch 13 iteration 3400 loss 2.167271852493286\n","Epoch 13 iteration 3500 loss 2.120739698410034\n","Epoch 13 iteration 3600 loss 2.365511417388916\n","Epoch 13 iteration 3700 loss 2.224036693572998\n","Epoch 13 iteration 3800 loss 2.6127052307128906\n","Epoch 13 Training loss 2.382643878113147\n","Epoch 14 iteration 0 loss 2.681400775909424\n","Epoch 14 iteration 100 loss 2.3374388217926025\n","Epoch 14 iteration 200 loss 2.165402412414551\n","Epoch 14 iteration 300 loss 2.071152448654175\n","Epoch 14 iteration 400 loss 2.535876512527466\n","Epoch 14 iteration 500 loss 2.665518045425415\n","Epoch 14 iteration 600 loss 2.595257043838501\n","Epoch 14 iteration 700 loss 2.6249959468841553\n","Epoch 14 iteration 800 loss 2.725299835205078\n","Epoch 14 iteration 900 loss 2.5947153568267822\n","Epoch 14 iteration 1000 loss 2.1103363037109375\n","Epoch 14 iteration 1100 loss 2.5596206188201904\n","Epoch 14 iteration 1200 loss 2.1798253059387207\n","Epoch 14 iteration 1300 loss 2.6242971420288086\n","Epoch 14 iteration 1400 loss 2.319913148880005\n","Epoch 14 iteration 1500 loss 2.5051677227020264\n","Epoch 14 iteration 1600 loss 2.1341712474823\n","Epoch 14 iteration 1700 loss 2.2459473609924316\n","Epoch 14 iteration 1800 loss 2.293029308319092\n","Epoch 14 iteration 1900 loss 2.3751204013824463\n","Epoch 14 iteration 2000 loss 2.0730948448181152\n","Epoch 14 iteration 2100 loss 2.467350959777832\n","Epoch 14 iteration 2200 loss 2.1372196674346924\n","Epoch 14 iteration 2300 loss 2.4863088130950928\n","Epoch 14 iteration 2400 loss 2.228363513946533\n","Epoch 14 iteration 2500 loss 2.4290475845336914\n","Epoch 14 iteration 2600 loss 2.215155839920044\n","Epoch 14 iteration 2700 loss 2.075808525085449\n","Epoch 14 iteration 2800 loss 2.4392073154449463\n","Epoch 14 iteration 2900 loss 2.027676582336426\n","Epoch 14 iteration 3000 loss 2.2281599044799805\n","Epoch 14 iteration 3100 loss 1.8932995796203613\n","Epoch 14 iteration 3200 loss 2.4478683471679688\n","Epoch 14 iteration 3300 loss 2.2842328548431396\n","Epoch 14 iteration 3400 loss 2.1618621349334717\n","Epoch 14 iteration 3500 loss 2.066175699234009\n","Epoch 14 iteration 3600 loss 2.294593572616577\n","Epoch 14 iteration 3700 loss 2.1975088119506836\n","Epoch 14 iteration 3800 loss 2.50289249420166\n","Epoch 14 Training loss 2.356759014664509\n","Epoch 15 iteration 0 loss 2.6153175830841064\n","Epoch 15 iteration 100 loss 2.2575459480285645\n","Epoch 15 iteration 200 loss 2.1390528678894043\n","Epoch 15 iteration 300 loss 2.117708444595337\n","Epoch 15 iteration 400 loss 2.3631491661071777\n","Epoch 15 iteration 500 loss 2.650949716567993\n","Epoch 15 iteration 600 loss 2.6244864463806152\n","Epoch 15 iteration 700 loss 2.6027026176452637\n","Epoch 15 iteration 800 loss 2.6430001258850098\n","Epoch 15 iteration 900 loss 2.5059831142425537\n","Epoch 15 iteration 1000 loss 2.08100962638855\n","Epoch 15 iteration 1100 loss 2.556654453277588\n","Epoch 15 iteration 1200 loss 2.15911602973938\n","Epoch 15 iteration 1300 loss 2.616178035736084\n","Epoch 15 iteration 1400 loss 2.3575692176818848\n","Epoch 15 iteration 1500 loss 2.444775104522705\n","Epoch 15 iteration 1600 loss 2.143432855606079\n","Epoch 15 iteration 1700 loss 2.2057764530181885\n","Epoch 15 iteration 1800 loss 2.22706937789917\n","Epoch 15 iteration 1900 loss 2.3014910221099854\n","Epoch 15 iteration 2000 loss 2.051729202270508\n","Epoch 15 iteration 2100 loss 2.439866065979004\n","Epoch 15 iteration 2200 loss 2.134282112121582\n","Epoch 15 iteration 2300 loss 2.4302055835723877\n","Epoch 15 iteration 2400 loss 2.1569983959198\n","Epoch 15 iteration 2500 loss 2.335343360900879\n","Epoch 15 iteration 2600 loss 2.0621824264526367\n","Epoch 15 iteration 2700 loss 2.139437675476074\n","Epoch 15 iteration 2800 loss 2.344196319580078\n","Epoch 15 iteration 2900 loss 1.987665057182312\n","Epoch 15 iteration 3000 loss 2.223646640777588\n","Epoch 15 iteration 3100 loss 1.9155943393707275\n","Epoch 15 iteration 3200 loss 2.471435546875\n","Epoch 15 iteration 3300 loss 2.24337100982666\n","Epoch 15 iteration 3400 loss 2.1118156909942627\n","Epoch 15 iteration 3500 loss 2.093078851699829\n","Epoch 15 iteration 3600 loss 2.1976540088653564\n","Epoch 15 iteration 3700 loss 2.1665637493133545\n","Epoch 15 iteration 3800 loss 2.523141860961914\n","Epoch 15 Training loss 2.33391482658264\n","Evaluation loss 3.1709862778462425\n","Epoch 16 iteration 0 loss 2.6765599250793457\n","Epoch 16 iteration 100 loss 2.3001890182495117\n","Epoch 16 iteration 200 loss 2.1423227787017822\n","Epoch 16 iteration 300 loss 2.031797409057617\n","Epoch 16 iteration 400 loss 2.398972749710083\n","Epoch 16 iteration 500 loss 2.5951051712036133\n","Epoch 16 iteration 600 loss 2.5543243885040283\n","Epoch 16 iteration 700 loss 2.5047059059143066\n","Epoch 16 iteration 800 loss 2.6275041103363037\n","Epoch 16 iteration 900 loss 2.520866632461548\n","Epoch 16 iteration 1000 loss 2.0125434398651123\n","Epoch 16 iteration 1100 loss 2.4905896186828613\n","Epoch 16 iteration 1200 loss 2.1841416358947754\n","Epoch 16 iteration 1300 loss 2.6254987716674805\n","Epoch 16 iteration 1400 loss 2.2845377922058105\n","Epoch 16 iteration 1500 loss 2.491914749145508\n","Epoch 16 iteration 1600 loss 2.1758100986480713\n","Epoch 16 iteration 1700 loss 2.1941983699798584\n","Epoch 16 iteration 1800 loss 2.2338366508483887\n","Epoch 16 iteration 1900 loss 2.3007092475891113\n","Epoch 16 iteration 2000 loss 2.0990819931030273\n","Epoch 16 iteration 2100 loss 2.3497231006622314\n","Epoch 16 iteration 2200 loss 2.150181770324707\n","Epoch 16 iteration 2300 loss 2.486204147338867\n","Epoch 16 iteration 2400 loss 2.145956516265869\n","Epoch 16 iteration 2500 loss 2.300396203994751\n","Epoch 16 iteration 2600 loss 2.069032669067383\n","Epoch 16 iteration 2700 loss 2.0167441368103027\n","Epoch 16 iteration 2800 loss 2.383763074874878\n","Epoch 16 iteration 2900 loss 2.0393877029418945\n","Epoch 16 iteration 3000 loss 2.323697566986084\n","Epoch 16 iteration 3100 loss 1.8223916292190552\n","Epoch 16 iteration 3200 loss 2.4032576084136963\n","Epoch 16 iteration 3300 loss 2.2258992195129395\n","Epoch 16 iteration 3400 loss 2.1269006729125977\n","Epoch 16 iteration 3500 loss 1.994917392730713\n","Epoch 16 iteration 3600 loss 2.259582042694092\n","Epoch 16 iteration 3700 loss 2.158860683441162\n","Epoch 16 iteration 3800 loss 2.4573535919189453\n","Epoch 16 Training loss 2.312977213853107\n","Epoch 17 iteration 0 loss 2.6900241374969482\n","Epoch 17 iteration 100 loss 2.1698813438415527\n","Epoch 17 iteration 200 loss 2.0988333225250244\n","Epoch 17 iteration 300 loss 2.0502471923828125\n","Epoch 17 iteration 400 loss 2.35658597946167\n","Epoch 17 iteration 500 loss 2.5927734375\n","Epoch 17 iteration 600 loss 2.5508673191070557\n","Epoch 17 iteration 700 loss 2.575852870941162\n","Epoch 17 iteration 800 loss 2.601236343383789\n","Epoch 17 iteration 900 loss 2.559485673904419\n","Epoch 17 iteration 1000 loss 2.028951406478882\n","Epoch 17 iteration 1100 loss 2.501396894454956\n","Epoch 17 iteration 1200 loss 2.1296732425689697\n","Epoch 17 iteration 1300 loss 2.586531639099121\n","Epoch 17 iteration 1400 loss 2.253899335861206\n","Epoch 17 iteration 1500 loss 2.497117519378662\n","Epoch 17 iteration 1600 loss 2.001558303833008\n","Epoch 17 iteration 1700 loss 2.1524112224578857\n","Epoch 17 iteration 1800 loss 2.291024684906006\n","Epoch 17 iteration 1900 loss 2.253952741622925\n","Epoch 17 iteration 2000 loss 2.0550339221954346\n","Epoch 17 iteration 2100 loss 2.398266077041626\n","Epoch 17 iteration 2200 loss 2.1451501846313477\n","Epoch 17 iteration 2300 loss 2.386136531829834\n","Epoch 17 iteration 2400 loss 2.1378350257873535\n","Epoch 17 iteration 2500 loss 2.415374994277954\n","Epoch 17 iteration 2600 loss 2.000110387802124\n","Epoch 17 iteration 2700 loss 1.985074520111084\n","Epoch 17 iteration 2800 loss 2.3078935146331787\n","Epoch 17 iteration 2900 loss 1.981025218963623\n","Epoch 17 iteration 3000 loss 2.1879405975341797\n","Epoch 17 iteration 3100 loss 1.8853269815444946\n","Epoch 17 iteration 3200 loss 2.2943921089172363\n","Epoch 17 iteration 3300 loss 2.2404236793518066\n","Epoch 17 iteration 3400 loss 2.078753709793091\n","Epoch 17 iteration 3500 loss 1.9936083555221558\n","Epoch 17 iteration 3600 loss 2.2410504817962646\n","Epoch 17 iteration 3700 loss 2.1872074604034424\n","Epoch 17 iteration 3800 loss 2.398947238922119\n","Epoch 17 Training loss 2.295027912894407\n","Epoch 18 iteration 0 loss 2.6354172229766846\n","Epoch 18 iteration 100 loss 2.2062487602233887\n","Epoch 18 iteration 200 loss 2.0907835960388184\n","Epoch 18 iteration 300 loss 2.0229146480560303\n","Epoch 18 iteration 400 loss 2.3240997791290283\n","Epoch 18 iteration 500 loss 2.5397918224334717\n","Epoch 18 iteration 600 loss 2.5439236164093018\n","Epoch 18 iteration 700 loss 2.4969825744628906\n","Epoch 18 iteration 800 loss 2.6222848892211914\n","Epoch 18 iteration 900 loss 2.493159770965576\n","Epoch 18 iteration 1000 loss 1.9875304698944092\n","Epoch 18 iteration 1100 loss 2.508265495300293\n","Epoch 18 iteration 1200 loss 2.10598087310791\n","Epoch 18 iteration 1300 loss 2.597377300262451\n","Epoch 18 iteration 1400 loss 2.195664882659912\n","Epoch 18 iteration 1500 loss 2.4393439292907715\n","Epoch 18 iteration 1600 loss 2.006864309310913\n","Epoch 18 iteration 1700 loss 2.188877820968628\n","Epoch 18 iteration 1800 loss 2.2535505294799805\n","Epoch 18 iteration 1900 loss 2.166243076324463\n","Epoch 18 iteration 2000 loss 2.0134072303771973\n","Epoch 18 iteration 2100 loss 2.383849859237671\n","Epoch 18 iteration 2200 loss 2.0493481159210205\n","Epoch 18 iteration 2300 loss 2.3978819847106934\n","Epoch 18 iteration 2400 loss 2.1103005409240723\n","Epoch 18 iteration 2500 loss 2.274700403213501\n","Epoch 18 iteration 2600 loss 2.085275411605835\n","Epoch 18 iteration 2700 loss 2.0176775455474854\n","Epoch 18 iteration 2800 loss 2.3342559337615967\n","Epoch 18 iteration 2900 loss 1.9016845226287842\n","Epoch 18 iteration 3000 loss 2.127678155899048\n","Epoch 18 iteration 3100 loss 1.8322633504867554\n","Epoch 18 iteration 3200 loss 2.3939998149871826\n","Epoch 18 iteration 3300 loss 2.1938955783843994\n","Epoch 18 iteration 3400 loss 2.1074397563934326\n","Epoch 18 iteration 3500 loss 2.070112943649292\n","Epoch 18 iteration 3600 loss 2.247450828552246\n","Epoch 18 iteration 3700 loss 2.1792523860931396\n","Epoch 18 iteration 3800 loss 2.499185562133789\n","Epoch 18 Training loss 2.2774312774309937\n","Epoch 19 iteration 0 loss 2.59816312789917\n","Epoch 19 iteration 100 loss 2.206038475036621\n","Epoch 19 iteration 200 loss 2.0031166076660156\n","Epoch 19 iteration 300 loss 1.9920947551727295\n","Epoch 19 iteration 400 loss 2.2847635746002197\n","Epoch 19 iteration 500 loss 2.598789691925049\n","Epoch 19 iteration 600 loss 2.5459048748016357\n","Epoch 19 iteration 700 loss 2.49950909614563\n","Epoch 19 iteration 800 loss 2.6435296535491943\n","Epoch 19 iteration 900 loss 2.476233959197998\n","Epoch 19 iteration 1000 loss 2.0442276000976562\n","Epoch 19 iteration 1100 loss 2.4061331748962402\n","Epoch 19 iteration 1200 loss 2.046139717102051\n","Epoch 19 iteration 1300 loss 2.586008310317993\n","Epoch 19 iteration 1400 loss 2.261209011077881\n","Epoch 19 iteration 1500 loss 2.3823940753936768\n","Epoch 19 iteration 1600 loss 2.0689566135406494\n","Epoch 19 iteration 1700 loss 2.1321372985839844\n","Epoch 19 iteration 1800 loss 2.169246196746826\n","Epoch 19 iteration 1900 loss 2.2439417839050293\n","Epoch 19 iteration 2000 loss 2.017714500427246\n","Epoch 19 iteration 2100 loss 2.3166496753692627\n","Epoch 19 iteration 2200 loss 2.03397274017334\n","Epoch 19 iteration 2300 loss 2.4094200134277344\n","Epoch 19 iteration 2400 loss 2.0932857990264893\n","Epoch 19 iteration 2500 loss 2.3056447505950928\n","Epoch 19 iteration 2600 loss 1.9650605916976929\n","Epoch 19 iteration 2700 loss 1.9973878860473633\n","Epoch 19 iteration 2800 loss 2.281191110610962\n","Epoch 19 iteration 2900 loss 1.954221487045288\n","Epoch 19 iteration 3000 loss 2.133925437927246\n","Epoch 19 iteration 3100 loss 1.7571985721588135\n","Epoch 19 iteration 3200 loss 2.3769524097442627\n","Epoch 19 iteration 3300 loss 2.2185869216918945\n","Epoch 19 iteration 3400 loss 2.0398976802825928\n","Epoch 19 iteration 3500 loss 2.0389962196350098\n","Epoch 19 iteration 3600 loss 2.284071445465088\n","Epoch 19 iteration 3700 loss 2.121265411376953\n","Epoch 19 iteration 3800 loss 2.4437036514282227\n","Epoch 19 Training loss 2.2623012243844585\n"]}],"source":["# 定义训练和验证函数\n","def evaluate(model, data):\n","    model.eval()\n","    total_num_words = total_loss = 0.\n","    with torch.no_grad():\n","        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n","            mb_x = torch.from_numpy(mb_x).to(device).long()    # 这个是一个batch的英文句子 大小是[batch_size, seq_len]\n","            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()    # 每个句子的长度\n","            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()  # decoder的输入，训练的时候是整个summary（带<GO>）\n","            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()   # 解码器那边的输出  [batch_size, seq_len-1]去掉<GO>\n","            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()  # 这个减去1， 去掉初始符号 [batch_size, seq_len-1]\n","            mb_y_len[mb_y_len<=0] =  1   # 以防出错，防止trainset里面的summary长度被记为0（实际上不可能）\n","            \n","            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n","            \n","            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]  \n","            # [batch_size,最长句子的长度], 上面是bool类型， 下面是float类型， 只计算每个句子的有效部分， 填充的那部分去掉\n","            mb_out_mask = mb_out_mask.float()  # [batch_size, seq_len-1]  因为mb_y_len.max()就是seq_len-1\n","            \n","            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n","            \n","            num_words = torch.sum(mb_y_len).item()\n","            total_loss += loss.item() * num_words\n","            total_num_words += num_words\n","    print('Evaluation loss', total_loss / total_num_words)\n","  \n","def train(model, data, num_epochs=20):\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_num_words = total_loss = 0.\n","        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n","            mb_x = torch.from_numpy(mb_x).to(device).long()\n","            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n","            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n","            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n","            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n","            mb_y_len[mb_y_len<=0] = 1\n","            \n","            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n","            \n","            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n","            mb_out_mask = mb_out_mask.float()\n","            \n","            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n","            \n","            num_words = torch.sum(mb_y_len).item()\n","            total_loss += loss.item() * num_words\n","            total_num_words += num_words\n","            \n","            # 更新\n","            optimizer.zero_grad()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)     # 防止梯度爆炸\n","            optimizer.step()\n","            \n","            if it % 100 == 0:\n","                print('Epoch', epoch, 'iteration', it, 'loss', loss.item())\n","            \n","        print('Epoch', epoch, 'Training loss', total_loss / total_num_words)\n","        if epoch % 5 == 0:\n","            evaluate(model, dev_data)\n","        \n","# 训练\n","train(model, train_data, num_epochs=20)\n"]},{"cell_type":"code","execution_count":152,"id":"80fe146d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80fe146d","executionInfo":{"status":"ok","timestamp":1670193830028,"user_tz":360,"elapsed":5,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"5fea2730-ceb2-4c3c-9870-65973fb176cc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["27259"]},"metadata":{},"execution_count":152}],"source":["len(text_val)"]},{"cell_type":"code","execution_count":153,"id":"1a2a88da","metadata":{"id":"1a2a88da","executionInfo":{"status":"ok","timestamp":1670193833917,"user_tz":360,"elapsed":322,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"outputs":[],"source":["def translate_dev(i):\n","    text = \" \".join([int_to_vocab[w] for w in text_val[i]])\n","    print(text)\n","    summary = \" \".join([int_to_vocab[w] for w in summ_val[i]])\n","    print(summary)\n","\n","    mb_x = torch.from_numpy(np.array(text_val[i]).reshape(1,-1)).long().to(device)\n","    mb_x_len = torch.from_numpy(np.array([len(text_val[i])])).long().to(device)\n","    # mb_y = torch.from_numpy(summ_val[i].reshape(1,-1)).long().to(device)\n","    # mb_y_len = torch.from_numpy([len(summ_val[i])]).long().to(device)\n","    bos = torch.Tensor([[vocab_to_int[\"<GO>\"]]]).long().to(device)\n","    # 翻译时decoder的输入序列bos： [<GO>的index] \n","    translation, attn = model.translate(mb_x,mb_x_len, bos)\n","    translation = [int_to_vocab[i] for i in translation.data.cpu().numpy().reshape(-1)]\n","    trans = []\n","    for word in translation:\n","        if word !=\"<EOS>\":\n","            trans.append(word)\n","        else:\n","            break\n","    print(\" \".join(trans))\n","    "]},{"cell_type":"code","execution_count":154,"id":"2c8ed020","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2c8ed020","executionInfo":{"status":"ok","timestamp":1670193838687,"user_tz":360,"elapsed":641,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"f06c12ad-814e-4015-d653-a3167795c720"},"outputs":[{"output_type":"stream","name":"stdout","text":["<GO> pleasantly surprised brought home used wanchai ferry box bought caught eye enjoy cooking chinese style food product comes everything need chicken pork find works best chicken comes variety tasty flavors <EOS>\n","<GO> tasty <EOS>\n","great rub\n","\n","<GO> cannot find sugar free caramels anywhere happened come across one grocery store regular store mission figure get regular basis good might cave buy amazon 12 pack soon <EOS>\n","<GO> really really good <EOS>\n","great deal\n","\n","<GO> happy tried product 2 3 dogs fussy one giving dog food altogether much people food looked dog food review site impressed opened first thrilled find small chunks pretty much diced pieces chiken vegetables gravy <UNK> find difficult find chunky type foods smaller dogs ones liked given loose stools liked food highly recommend <EOS>\n","<GO> all 3 dogs will eat this <EOS>\n","great for dogs with allergies\n","\n","<GO> amazon would get ball offer cereal free shipping would order regularly best gluten free hot cereal reminds eating cream wheat malt meal pay shipping better buying store come amazon <EOS>\n","<GO> great hot cereal <EOS>\n","gluten free cereal\n","\n","<GO> super yummy found one starbucks immediately ordered several cases amazon little guy loves older kids perfect snack knock around diaper bag without fear getting bruised like fresh fruit liked much considering upgrading amazon prime stocking enjoy <EOS>\n","<GO> yum yum yum <EOS>\n","yummy\n","\n","<GO> 7 year old lab hip issues last year powder seems helped mobility really like taste eats food powder almost begrudgingly noticed since started using powder seems active playful months prior <br >that said powder inconvenient bit messy mention fact dog enjoy tablet even liquid like tincture form would five star review <EOS>\n","<GO> great results a little inconvenient <EOS>\n","great dog food\n","\n","<GO> dog take seizure pills 2 months shoving throat finding pills around house saw pill pockets vet likes pills get spit get growled great pills tiny split one pill pocket wrap four pills put extras empty pill bottle keep fresh mind smaller size leave hands kind greasy though <EOS>\n","<GO> terrific idea <EOS>\n","greenies pill pockets\n","\n","<GO> really hate give food one star 4 cats simply touch love nature organics canned food eat flavor nature organics dry foods want feed cats food less fillers sticking royal canin science diet <br ><br >it would great trial sizes dry food testing flavors costly <EOS>\n","<GO> great quality but they will not eat it <EOS>\n","cats love it\n","\n","<GO> got thinking would good healthy snack indeed taste pretty good way oily expected like veggie chips really like <br ><br >hey might healthy matter leave greasy residue mouth throat eating handful bad french fries <EOS>\n","<GO> too oily greasy <EOS>\n","not bad\n","\n","<GO> surprised great cocoa milk chocolate right amount chocolate rich tasting mint chocolate right amount mint also rich dark chocolate smooth tasting real heavy taste chocolate happy variety box <br >k davis <EOS>\n","<GO> grove square hot cocoa variety pack <EOS>\n","wonderful cocoa\n","\n","<GO> best gluten free chocolate cake mix ever tried nobody suspects gluten free may remember called danielle decadent chocolate cake mix glutino purchased gluten free pantry company moist delicious <EOS>\n","<GO> excellent chocolate cake <EOS>\n","best gluten free chocolate chip cookies\n","\n","<GO> bought fiber one bars couple hours ago completely satisfied taste actually surprised tasted good definitely purchase future side many carbs taking one day times need something sweet perfect <EOS>\n","<GO> great tasting <EOS>\n","very good\n","\n","<GO> dark chocolate covered cherries super good eat healthy trouble finding health food stores live decided order online live hot climate company packed cool packs would melt never seen company go effort would highly recommend product anyone loves dried cherries dark chocolate looking healthy treat <EOS>\n","<GO> super good <EOS>\n","great product\n","\n","<GO> like green mountain nantucket blend often bought whole bean ground form brew standard coffeemaker vue version seems bit different less flavorful aromatic fact bit disappointing nantucket iced version excellent perhaps ice kills taste buds bit hot version left cold <EOS>\n","<GO> good mid roast blend but not particularly flavorful <EOS>\n","not as good as the reviews\n","\n","<GO> purchased item thinking label sweet potato large letters front bag actually sweet potato chips find eating finding taste terrible tortilla chips dusting sweet potato dust <EOS>\n","<GO> terrible <EOS>\n","terrible\n","\n","<GO> far one best k cup coffees <br >the favor bold yet bitter weak diluted even choosing largest cup size keurig machine <br >only issue pick ms 36 80 pc pack <EOS>\n","<GO> one of the best <EOS>\n","a lovely cup of coffee\n","\n","<GO> rendezvous restaurant memphis good place go like barbecued ribs glad sell barbecue sauce seasoning amazon since live memphis thus cannot stop restaurant often would like would sell ship ribs amazon <EOS>\n","<GO> rib tickling good <EOS>\n","best bbq sauce\n","\n","<GO> water look great tastes terrible think tastes like bottled old water found sitting stump also grammar description back bottle incorrect maybe fools bottling old water fool bought <EOS>\n","<GO> blk <EOS>\n","tastes horrible\n","\n","<GO> woke morning intent making brunch baked grits based recipe modified bit accommodate polenta de la estancia heartbroken discover less 1 2 cup left <br ><br >i like polenta grits cultural variation thereof tasted polenta de la estancia right combination corn flavor texture aroma make mouth water made chicken stock herbs great substitute mash potatoes hold great steak even meatballs marinara sauce made cheese 2 milk wonderful foundation eggs bacon <br ><br >buy regret <EOS>\n","<GO> i love this <EOS>\n","the best\n","\n","<GO> buy happy hips treats 14 year old husky loves great need tuck pill easy medication dose best treats contain glucosamine chondroitin movement improves time <EOS>\n","<GO> happy hips for a happy dog <EOS>\n","great treats for the active dog\n","\n"]}],"source":["for i in range(100,120):\n","    translate_dev(i)\n","    print()"]},{"cell_type":"code","execution_count":155,"id":"52da9613","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"52da9613","executionInfo":{"status":"ok","timestamp":1670193866514,"user_tz":360,"elapsed":359,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"c13f7015-0083-4e19-b68d-78b5ab2a0284"},"outputs":[{"output_type":"stream","name":"stdout","text":["<GO> amazing turn great every time taste better overpriced prepackaged ones let family share healthy treat instead keeping definitely repeat purchase buy least month <EOS>\n","<GO> amazing <EOS>\n","great taste\n"]}],"source":["translate_dev(150)"]},{"cell_type":"code","execution_count":118,"id":"c2a79058","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"c2a79058","executionInfo":{"status":"error","timestamp":1670188216921,"user_tz":360,"elapsed":339,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"d1f8fb23-81e9-4216-ee5c-08a7aeac5dbd"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-118-6f5a16864d15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./model/checkpoint.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model/checkpoint.pt'"]}],"source":["torch.save(model.state_dict(), './model/checkpoint.pt')"]},{"cell_type":"code","execution_count":null,"id":"7c829d3f","metadata":{"id":"7c829d3f"},"outputs":[],"source":["model.load_state_dict(torch.load('./model/checkpoint.pt'))"]},{"cell_type":"code","execution_count":156,"id":"6cba5bb3","metadata":{"id":"6cba5bb3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670193889172,"user_tz":360,"elapsed":838,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"edd899c0-2f6c-4afe-f871-418a8827d4af"},"outputs":[{"output_type":"stream","name":"stdout","text":["<GO> found cereal month ago become one favorite snacks counting calories familiar weight watchers 2 points cup incredible cereal let alone one tastes great yum love snacks bowl almond milk rocks <EOS>\n","<GO> 2 points per cup <EOS>\n","the best snack ever\n"]}],"source":["translate_dev(7)"]},{"cell_type":"code","source":["!pip install rouge"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JPUODawUB6Oy","executionInfo":{"status":"ok","timestamp":1670189169472,"user_tz":360,"elapsed":4632,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"41860573-07ec-41a7-9b1b-8c19f590c0db"},"id":"JPUODawUB6Oy","execution_count":123,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting rouge\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from rouge) (1.15.0)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.1\n"]}]},{"cell_type":"code","source":["from rouge import Rouge"],"metadata":{"id":"Q6-qU6qiBvi1","executionInfo":{"status":"ok","timestamp":1670193908443,"user_tz":360,"elapsed":309,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}}},"id":"Q6-qU6qiBvi1","execution_count":157,"outputs":[]},{"cell_type":"code","source":["label_summary_list = []\n","summary_list = []\n","for i in range(len(text_val)):\n","    mb_x = torch.from_numpy(np.array(text_val[i]).reshape(1,-1)).long().to(device)\n","    mb_x_len = torch.from_numpy(np.array([len(text_val[i])])).long().to(device)\n","    y = summ_val[i]\n","    label_summary = \" \".join(int_to_vocab[i] for i in y[1:-1])\n","\n","    bos = torch.Tensor([[vocab_to_int[\"<GO>\"]]]).long().to(device)\n","    translation, attn = model.translate(mb_x, mb_x_len, bos)\n","    translation = [int_to_vocab[i] for i in translation.data.cpu().numpy().reshape(-1)]\n","    trans = []\n","    for word in translation:\n","        if word !=\"<EOS>\":\n","            trans.append(word)\n","        else:\n","            break\n","    trans = \" \".join(trans)\n","    if (len(trans)>0 and len(label_summary)>0):\n","        label_summary_list.append(label_summary)\n","        summary_list.append(trans)\n","\n","rouge = Rouge()\n","rouge_score = rouge.get_scores(label_summary_list, summary_list, avg=True)\n","print(\"rouge-1:\",rouge_score[\"rouge-1\"])\n","print(\"rouge-2:\",rouge_score[\"rouge-2\"])\n","print(\"rouge-l:\",rouge_score[\"rouge-l\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ndqdbI--oPo","executionInfo":{"status":"ok","timestamp":1670194316371,"user_tz":360,"elapsed":403819,"user":{"displayName":"Xiaozhu Jin","userId":"03153064271904421143"}},"outputId":"eb5b9054-dd96-4ab9-d2b4-66f599c5c1b2"},"id":"7ndqdbI--oPo","execution_count":158,"outputs":[{"output_type":"stream","name":"stdout","text":["rouge-1: {'r': 0.27043780379987475, 'p': 0.22337165170877343, 'f': 0.23230628476619672}\n","rouge-2: {'r': 0.13760870692055135, 'p': 0.1190969055365106, 'f': 0.12233730355425801}\n","rouge-l: {'r': 0.26839591625402426, 'p': 0.2220034582476763, 'f': 0.23077897826035584}\n"]}]},{"cell_type":"markdown","id":"50c8d79a","metadata":{"id":"50c8d79a"},"source":["#### below are abandoned code"]},{"cell_type":"code","execution_count":null,"id":"e7b326b4","metadata":{"id":"e7b326b4"},"outputs":[],"source":["# class MyDataset(Dataset):\n","#   def __init__(self,sorted_texts,sorted_summaries, transform) -> None:\n","#     self.texts_set = sorted_texts\n","#     self.summaries_set = sorted_summaries\n","#     self.transform = transform\n","    \n","#     assert(len(self.texts_set)==len(self.summaries_set))\n","\n","#   def __len__(self):\n","#     return len(self.texts_set)\n","\n","#   def __getitem__(self, idx):\n","\n","#     return (torch.Tensor(self.texts_set[idx]),torch.Tensor(self.summaries_set[idx]) )"]},{"cell_type":"code","execution_count":null,"id":"91e1ae2a","metadata":{"pycharm":{"name":"#%%\n"},"id":"91e1ae2a"},"outputs":[],"source":["# def collate_fn(data):\n","#     data.sort(key=lambda x: len(x[0]),reverse=True)\n","#     x = [i[0] for i in data]\n","#     x_lenth = [len(i)for i in x]\n","#     y = [i[1] for i in data]\n","#     y_lenth = [len(i)for i in y]\n","#     text = rnn_utils.pad_sequence(x, batch_first=True, padding_value=0)\n","#     summary = rnn_utils.pad_sequence(y, batch_first=True, padding_value=0)\n","#     return text.long(),x_lenth,summary.long(),y_lenth\n"]},{"cell_type":"code","execution_count":null,"id":"a27fe3a5","metadata":{"pycharm":{"name":"#%%\n"},"id":"a27fe3a5"},"outputs":[],"source":["# all_dataset = MyDataset(sorted_texts, sorted_summaries, None)\n","# train_size = int(len(all_dataset) * 0.9)\n","# validate_size = int(len(all_dataset) * 0.05)\n","# test_size = len(all_dataset)-train_size-validate_size\n","# train_dataset, validate_dataset, test_dataset = torch.utils.data.random_split(all_dataset, [train_size, validate_size, test_size])\n","# train_loader = DataLoader(train_dataset, batch_size=5000, shuffle=False, collate_fn=collate_fn)\n","# validate_loader = DataLoader(validate_dataset, batch_size=500, shuffle=False, collate_fn=collate_fn)\n","# test_loader = DataLoader(test_dataset, batch_size=500, shuffle=False, collate_fn=collate_fn)\n","\n","\n","# print(len(train_loader))\n","# print(len(validate_loader))\n","# print(len(test_loader))\n"]},{"cell_type":"code","execution_count":null,"id":"cbdd48ca","metadata":{"pycharm":{"name":"#%%\n"},"id":"cbdd48ca"},"outputs":[],"source":["# class Seq2seq(nn.Module):\n","#     def __init__(self, input_size, hidden_size = 256, num_layers = 2, drop_pro = 0.25):\n","#         super(Seq2seq, self).__init__()\n","#         self.encoding_layer = nn.LSTM(\n","#             input_size=input_size,\n","#             hidden_size=hidden_size,\n","#             num_layers=num_layers,\n","#             batch_first=True,\n","#             dropout = drop_pro,\n","#             bidirectional = True\n","#         )\n","#         self.decoding_layer = nn.LSTM(\n","#             input_size=input_size,\n","#             hidden_size=hidden_size,\n","#             num_layers=num_layers,\n","#             batch_first=True,\n","#             dropout = drop_pro,\n","#         )\n","        \n","\n","\n","#     def forward(self, x, y):\n","#          output, output_state = self.encoding_layer(x)\n","#          print(output)  \n","         \n","#        out, hidden_prev = self.rnn(x, hidden_prev)\n","#        # [b, seq, h]\n","#        out = out.view(-1, hidden_size)\n","#        out = self.linear(out)#[seq,h] => [seq,3]\n","#        out = out.unsqueeze(dim=0)  # => [1,seq,3]\n","#        return out, hidden_prev\n","    \n","  \n","        \n"]},{"cell_type":"code","execution_count":null,"id":"9a78d9c6","metadata":{"pycharm":{"name":"#%%\n"},"id":"9a78d9c6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"91c7b423","metadata":{"pycharm":{"name":"#%%\n"},"id":"91c7b423"},"outputs":[],"source":["# epochs = 20\n","# batch_size = 16\n","# rnn_size = 256 \n","# num_layers = 2\n","# learning_rate = 0.005\n","# drop_out = 0.25\n","\n","# def fit(train_loader,epochs = 1, lr=0.001, drop_out=0.25):\n","#     embedding = torch.Tensor(word_embedding_matrix)\n","#     model = Seq2seq(embedding_dim,rnn_size, num_layers ,drop_out)\n","#     for epoch in range(epochs): \n","#         train_losses = []\n","#         for batch in train_loader:\n","#             text,t_len,summary,s_len = batch\n","#             text = F.embedding(text, embedding)\n","#             text_pack = rnn_utils.pack_padded_sequence(text, t_len, batch_first=True, enforce_sorted = False)\n","            \n","# #Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n","#             go_pad = torch.full([summary.size()[0],1],  vocab_to_int['<GO>'])\n","#             summary = torch.cat((go_pad,summary),1)\n","#             summary = F.embedding(summary, embedding)\n","#             summary_pack = rnn_utils.pack_padded_sequence(summary, s_len, batch_first=True,  enforce_sorted = False)\n","#             model(text_pack, summary_pack)\n","\n","#             break\n","#         break"]},{"cell_type":"code","execution_count":null,"id":"17c3f01d","metadata":{"pycharm":{"name":"#%%\n"},"id":"17c3f01d"},"outputs":[],"source":["# fit(train_loader)"]},{"cell_type":"code","execution_count":null,"id":"0f77f7ca","metadata":{"pycharm":{"name":"#%%\n"},"id":"0f77f7ca"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"collapsed_sections":["50c8d79a"]},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}